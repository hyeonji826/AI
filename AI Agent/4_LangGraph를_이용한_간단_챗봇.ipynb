{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf8FJzAvByGEyJO9njUR0z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonji826/AI/blob/main/AI%20Agent/4_LangGraph%EB%A5%BC_%EC%9D%B4%EC%9A%A9%ED%95%9C_%EA%B0%84%EB%8B%A8_%EC%B1%97%EB%B4%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Tool Calling Agent\n",
        "[Tool Calling Agent](https://python.langchain.com/docs/integrations/tools/)는 자신이 가진 지식만 사용하는 것이 아니라, 외부 도구(API, 데이터베이스, 코드 실행기 등)를 호출해 문제를 해결하는 에이전트입니다. 사용자의 질문을 이해한 뒤 필요한 경우 적절한 툴을 선택하고, 입력값을 구성해 호출하며, 반환된 결과를 다시 가공해 최종 답변을 만듭니다. 쉽게 말해, 단순히 대화만 하는 AI가 아니라 “필요할 때 계산기, 검색엔진, 데이터 조회 도구 같은 도구를 직접 쓸 수 있는 AI”가 Tool Calling Agent입니다.\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dna/bpNdR2/btsQsJsQfs0/AAAAAAAAAAAAAAAAAAAAANRBb48fj-_u3r63lFyG30rgOBjXxjUAxaNqRYIZi3I9/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=D6gQCjVI9%2FO%2Bphms7G2rxxuMS6g%3D\">"
      ],
      "metadata": {
        "id": "sY48KOrg-SGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 웹 검색을 하는 챗봇"
      ],
      "metadata": {
        "id": "cD9EMNuB-aiM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Tavily\n",
        "[Tavily](https://www.tavily.com/)는 웹을 실시간으로 검색해 AI가 최신·정확한 정보를 답변할 수 있도록 돕는 AI용 검색·브라우징 API 플랫폼입니다.\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dna/bcG6gs/btsQsLqGsc8/AAAAAAAAAAAAAAAAAAAAAN4N21J1DDr6nFhRcwjzTgjrx13VEOiWBjhU4H-YhM1g/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=lNxGOyEWvCKp68kQeYlL9gMzgU8%3D\">"
      ],
      "metadata": {
        "id": "x22QbYKM-bVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tavily-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkpcO9ev-jpq",
        "outputId": "a413c4c9-e052-430d-93d5-bbc377b43f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tavily-python\n",
            "  Downloading tavily_python-0.7.11-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.4)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.11.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.28.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (2025.8.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->tavily-python) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->tavily-python) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->tavily-python) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->tavily-python) (4.15.0)\n",
            "Downloading tavily_python-0.7.11-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: tavily-python\n",
            "Successfully installed tavily-python-0.7.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from tavily import TavilyClient\n",
        "from langchain_tavily import TavilySearch"
      ],
      "metadata": {
        "id": "fZVQIUD1BG7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "_set_env(\"TAVILY_API_KEY\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpcL2Kb1BNx1",
        "outputId": "d0bd9c73-0e09-46ff-af74-4e9f0de0ac93"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "TAVILY_API_KEY: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tavily_client = TavilyClient()"
      ],
      "metadata": {
        "id": "2uGTwhVoBrJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = tavily_client.search(\"What is AI Agent?\", max_results=3) # , topic=\"news\", days = 10\n",
        "response\n",
        "\n",
        "# max_results=3  → 검색 결과의 최대 개수를 지정합니다. 3이므로, 검색 결과를 최대 3개까지 가져옵니다.\n",
        "# topic=\"news\" → 뉴스 기사 위주로 검색\n",
        "# days=10 → 최근 10일 이내의 자료만 검색"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDYoNEcNB_-l",
        "outputId": "94f08467-3294-46b1-dc9f-a2435f85b8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is AI Agent?',\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://aws.amazon.com/what-is/ai-agents/',\n",
              "   'title': 'What are AI Agents? - Artificial Intelligence - AWS',\n",
              "   'content': \"# What are AI Agents? What are AI Agents? ## What are AI Agents? An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use that data to perform self-directed tasks that meet predetermined goals. Humans set goals, but an AI agent independently chooses the best actions it needs to perform to achieve those goals. AI agents interact with their environment by collecting data through sensors or digital inputs. The AI agent applies the data to make an informed decision. AI agents can improve your business operations and your customers' experiences. AI agents require information to execute tasks they have planned successfully. With sufficient data, the AI agent methodically implements the task at hand.\",\n",
              "   'score': 0.94989765,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://github.com/resources/articles/ai/what-are-ai-agents',\n",
              "   'title': 'What are AI agents? - GitHub',\n",
              "   'content': 'Feb 6, 2025·AI agents are autonomous software tools that perform tasks, make decisions, and interact with their environment intelligently and rationally.',\n",
              "   'score': 0.9317675,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://www.salesforce.com/agentforce/ai-agents/',\n",
              "   'title': 'AI Agents: Definition, Types, Examples | Salesforce US',\n",
              "   'content': '# What Are AI Agents? AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. The adoption of AI agents offers numerous benefits, transforming how businesses interact with their customers and manage their service operations. AI agents offer numerous benefits, including improved productivity, reduced costs, enhanced decision-making, and a better customer experience. Use this information to make continuous improvements to your AI agents, ensuring they remain effective and relevant. Drawing from unified customer data, an agentic AI can surface relevant insights for your human agents, tailoring financial recommendations to each customer’s needs and goals.',\n",
              "   'score': 0.93161833,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 0.77,\n",
              " 'request_id': 'c092222c-0abf-420d-8ef1-fbfc6a2f9689'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['results']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS5_f4fwCGuL",
        "outputId": "9b9536ca-6f8e-455f-c506-ef731f96ef4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://aws.amazon.com/what-is/ai-agents/',\n",
              "  'title': 'What are AI Agents? - Artificial Intelligence - AWS',\n",
              "  'content': \"# What are AI Agents? What are AI Agents? ## What are AI Agents? An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use that data to perform self-directed tasks that meet predetermined goals. Humans set goals, but an AI agent independently chooses the best actions it needs to perform to achieve those goals. AI agents interact with their environment by collecting data through sensors or digital inputs. The AI agent applies the data to make an informed decision. AI agents can improve your business operations and your customers' experiences. AI agents require information to execute tasks they have planned successfully. With sufficient data, the AI agent methodically implements the task at hand.\",\n",
              "  'score': 0.94989765,\n",
              "  'raw_content': None},\n",
              " {'url': 'https://github.com/resources/articles/ai/what-are-ai-agents',\n",
              "  'title': 'What are AI agents? - GitHub',\n",
              "  'content': 'Feb 6, 2025·AI agents are autonomous software tools that perform tasks, make decisions, and interact with their environment intelligently and rationally.',\n",
              "  'score': 0.9317675,\n",
              "  'raw_content': None},\n",
              " {'url': 'https://www.salesforce.com/agentforce/ai-agents/',\n",
              "  'title': 'AI Agents: Definition, Types, Examples | Salesforce US',\n",
              "  'content': '# What Are AI Agents? AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. The adoption of AI agents offers numerous benefits, transforming how businesses interact with their customers and manage their service operations. AI agents offer numerous benefits, including improved productivity, reduced costs, enhanced decision-making, and a better customer experience. Use this information to make continuous improvements to your AI agents, ensuring they remain effective and relevant. Drawing from unified customer data, an agentic AI can surface relevant insights for your human agents, tailoring financial recommendations to each customer’s needs and goals.',\n",
              "  'score': 0.93161833,\n",
              "  'raw_content': None}]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get_search_context: 보통 문자열(string) 형태이며, 여러 개의 검색 결과에서 중요한 내용만 추려서 한 덩어리의 텍스트로 제공합니다.\n",
        "context = tavily_client.get_search_context(query=\"What is AI Agent?\")\n",
        "context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "7BBIIWBeDgOL",
        "outputId": "157bc2fc-ddc8-48b8-b725-8415d28452d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[{\"url\": \"https://aws.amazon.com/what-is/ai-agents/\", \"content\": \"# What are AI Agents? What are AI Agents? ## What are AI Agents? An artificial intelligence (AI) agent is a software program that can interact with its environment, collect data, and use that data to perform self-directed tasks that meet predetermined goals. Humans set goals, but an AI agent independently chooses the best actions it needs to perform to achieve those goals. AI agents interact with their environment by collecting data through sensors or digital inputs. The AI agent applies the data to make an informed decision. AI agents can improve your business operations and your customers\\' experiences. AI agents require information to execute tasks they have planned successfully. With sufficient data, the AI agent methodically implements the task at hand.\"}, {\"url\": \"https://github.com/resources/articles/ai/what-are-ai-agents\", \"content\": \"Feb 6, 2025\\\\u00b7AI agents are autonomous software tools that perform tasks, make decisions, and interact with their environment intelligently and rationally.\"}, {\"url\": \"https://www.salesforce.com/agentforce/ai-agents/\", \"content\": \"# What Are AI Agents? AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. AI agents are a type of artificial intelligence (AI) system that can understand and respond to customer inquiries without human intervention. The adoption of AI agents offers numerous benefits, transforming how businesses interact with their customers and manage their service operations. AI agents offer numerous benefits, including improved productivity, reduced costs, enhanced decision-making, and a better customer experience. Use this information to make continuous improvements to your AI agents, ensuring they remain effective and relevant. Drawing from unified customer data, an agentic AI can surface relevant insights for your human agents, tailoring financial recommendations to each customer\\\\u2019s needs and goals.\"}, {\"url\": \"https://www.ibm.com/think/topics/ai-agents\", \"content\": \"An artificial intelligence (AI) agent refers to a system or program that is capable of autonomously performing tasks on behalf of a user or another system.Agents vs. AI Assistants\\\\u00b7What are Components of AI...\\\\u00b7What Is Agentic AI?\"}, {\"url\": \"https://cloud.google.com/discover/what-are-ai-agents\", \"content\": \"AI agents are software systems that use AI to pursue goals and complete tasks on behalf of users. They show reasoning, planning, and memory.\"}]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# qna_search: Tavily가 반환한 최종 답변. 보통 문자열(string) 형태이며, 한두 문장 정도로 정리된 응답을 제공합니다.\n",
        "answer = tavily_client.qna_search(query=\"What is AI Agent?\")\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "uhFgYCZ0Disu",
        "outputId": "868bd2ec-fec4-4f9a-937a-6a675b77bfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An AI agent is an autonomous software program that performs tasks and makes decisions based on data. It can automate complex workflows and interact with its environment. AI agents use machine learning to improve over time.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TavilySearch\n",
        "\n",
        "https://python.langchain.com/docs/integrations/tools/tavily_search/\n",
        "\n",
        "```\n",
        "파라미터\n",
        "\n",
        "max_results (optional, int): 검색 결과 반환 수\n",
        "topic (optional, str): 검색 카테고리 / \"general\"(Default), \"news\", \"finance\"\n",
        "include_answer (optional, bool): 쿼리에 대한 답변 포함 여부\n",
        "include_raw_content (optional, bool): 결과 HTML 포함 여부\n",
        "include_images (optional, bool): 쿼리 관련 이미지 목록 포함 여부\n",
        "include_image_descriptions (optional, bool): 각 이미지에 대한 설명 텍스트 포함 여부\n",
        "search_depth (optional, str): 검색 깊이 / \"basic\"(Default),\"advanced\"\n",
        "time_range (optional, str): 필터링 날짜 범위 - \"day\", \"week\", \"month\", \"year\"\n",
        "include_domains (optional, List[str]): 구체적으로 포함할 도메인 목록\n",
        "exclude_domains (optional, List[str]): 구체적으로 제외할 도메인 목록\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "llP42DkTDl__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_tavily"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Aqdy8vtUD1-3",
        "outputId": "0aecf72d-759b-4213-aa72-e7d346ecc876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_tavily\n",
            "  Downloading langchain_tavily-0.2.11-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (3.12.15)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.20 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (0.3.75)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.12/dist-packages (from langchain_tavily) (2.32.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.11.14->langchain_tavily) (1.20.1)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain_tavily) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain_tavily) (0.4.24)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain_tavily) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain_tavily) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain<0.4.0,>=0.3.20->langchain_tavily) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_tavily) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_tavily) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_tavily) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain_tavily) (25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.3->langchain_tavily) (2025.8.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_tavily) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain_tavily) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain_tavily) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.20->langchain_tavily) (0.4.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.4.0,>=0.3.20->langchain_tavily) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain<0.4.0,>=0.3.20->langchain_tavily) (1.3.1)\n",
            "Downloading langchain_tavily-0.2.11-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: langchain_tavily\n",
            "Successfully installed langchain_tavily-0.2.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearch(max_results=3)\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XhUfCM_TD8bl",
        "outputId": "a6f1a720-31bd-43e2-e6de-f0a604192c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's a 'node' in LangGraph?\",\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n",
              "   'title': 'What is LangGraph? - IBM',\n",
              "   'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.',\n",
              "   'score': 0.9159971,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://langchain-ai.github.io/langgraph/concepts/low_level/',\n",
              "   'title': 'state graph node - Overview',\n",
              "   'content': 'In LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments: state : The state of the graph; config : A',\n",
              "   'score': 0.8923472,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://www.ionio.ai/blog/a-comprehensive-guide-about-langgraph-code-included',\n",
              "   'title': 'A Comprehensive Guide About Langgraph: Code Included - Ionio',\n",
              "   'content': 'A node can be any function or tool your agent uses in langgraph and these nodes are connected with other nodes using edges. Every workflow ends with a “END”',\n",
              "   'score': 0.89166987,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 1.29,\n",
              " 'request_id': '4ab46396-c2e8-4112-893e-95d832451e66'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_with_toolcall = tool.invoke({\"args\": {'query': \"What's a 'node' in LangGraph?\"}, \"type\": \"tool_call\", \"id\": \"foo\", \"name\": \"tavily_search\"})\n",
        "invoke_with_toolcall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FKHjI2I7HFjQ",
        "outputId": "1d661ba8-3cae-4ce2-b3d0-97403f170caa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMessage(content='{\"query\": \"What\\'s a \\'node\\' in LangGraph?\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.9159971, \"raw_content\": null}, {\"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"content\": \"LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. The core concepts of LangGraph include: graph structure, state management, and coordination. In LangGraph, each node represents an LLM agent, and the edges are the communication channels between these agents. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph allows you to create custom node types to implement complex agent logic.\", \"score\": 0.8259413, \"raw_content\": null}, {\"url\": \"https://medium.com/@kbdhunga/beginners-guide-to-langgraph-understanding-state-nodes-and-edges-part-1-897e6114fa48\", \"title\": \"Beginner\\'s Guide to LangGraph: Understanding State, Nodes, and ...\", \"content\": \"Jul 27, 2024·Nodes: Nodes are the fundamental building blocks of a graph. Each node represents a specific function or operation that processes the current\", \"score\": 0.8098936, \"raw_content\": null}], \"response_time\": 1.43, \"request_id\": \"c0198098-5234-415c-97f1-f3ecf4a6d939\"}', name='tavily_search', tool_call_id='foo')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_with_toolcall.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "vtL3mWpnHI4m",
        "outputId": "ad98869e-80a4-4012-f9ec-13fa08195d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"query\": \"What\\'s a \\'node\\' in LangGraph?\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.9159971, \"raw_content\": null}, {\"url\": \"https://www.datacamp.com/tutorial/langgraph-tutorial\", \"title\": \"LangGraph Tutorial: What Is LangGraph and How to Use It?\", \"content\": \"LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. LangGraph provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured manner. The core concepts of LangGraph include: graph structure, state management, and coordination. In LangGraph, each node represents an LLM agent, and the edges are the communication channels between these agents. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph allows you to create custom node types to implement complex agent logic.\", \"score\": 0.8259413, \"raw_content\": null}, {\"url\": \"https://medium.com/@kbdhunga/beginners-guide-to-langgraph-understanding-state-nodes-and-edges-part-1-897e6114fa48\", \"title\": \"Beginner\\'s Guide to LangGraph: Understanding State, Nodes, and ...\", \"content\": \"Jul 27, 2024·Nodes: Nodes are the fundamental building blocks of a graph. Each node represents a specific function or operation that processes the current\", \"score\": 0.8098936, \"raw_content\": null}], \"response_time\": 1.43, \"request_id\": \"c0198098-5234-415c-97f1-f3ecf4a6d939\"}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2dm3PaJ4HKeo",
        "outputId": "2b8a4a26-e1e2-4269-b3cc-01057904f552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.29-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.75)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.43)\n",
            "Collecting requests<3,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.24)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain_community) (0.24.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.32.5->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<2.0.0,>=0.3.75->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.29-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain_community-0.3.29 marshmallow-3.26.1 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              },
              "id": "32c593e1db5f4e15889d6b83b0d6e9c1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tool.invoke(\"What's a 'node' in LangGraph?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-JyiAtLHN28",
        "outputId": "1f0c090e-620e-4cb5-a319-3713ad8ee2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1835957720.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
            "  tool = TavilySearchResults(max_results=2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'What is LangGraph?',\n",
              "  'url': 'https://www.ibm.com/think/topics/langgraph',\n",
              "  'content': 'Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as “actors” that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.',\n",
              "  'score': 0.9257218},\n",
              " {'title': 'LangGraph Tutorial with Practical Example',\n",
              "  'url': 'https://www.gettingstarted.ai/langgraph-tutorial-with-example/',\n",
              "  'content': 'Nodes are defined as Python functions that can perform a set of actions. For example, a node can integrate with a large language model, process information, call an external API, or any other task.\\n\\nA LangGraph node takes the state of the graph as a parameter and returns an updated state after it is executed.\\n\\nHere\\'s an example of a basic node, a Python function:\\n\\n```\\nfrom langchain_openai import ChatOpenAI\\n\\n...\\n\\nllm = ChatOpenAI(model_name=\"gpt-4o\")',\n",
              "  'score': 0.898277}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://blog.kakaocdn.net/dna/6baZN/btsQsRRXzfR/AAAAAAAAAAAAAAAAAAAAAMhD5n8e45CuIY3PX4j1h2QZ22M3NoS_v-L3xsoowPds/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=c%2Fj%2FFrK3MrgaLn7NKAmBLvS%2Fnb0%3D\">"
      ],
      "metadata": {
        "id": "kDkBDMpGHRie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_with_toolcall = tool.invoke({\"args\": {'query': \"What's a 'node' in LangGraph?\"}, \"type\": \"tool_call\", \"id\": \"foo\", \"name\": \"tavily\"})\n",
        "invoke_with_toolcall"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dW31IPf9HUzk",
        "outputId": "baf9d183-1e93-4fb5-e24a-c7cb76bcd878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ToolMessage(content='[{\"title\": \"What is LangGraph?\", \"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as “actors” that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.\", \"score\": 0.9257218}, {\"title\": \"LangGraph Tutorial with Practical Example\", \"url\": \"https://www.gettingstarted.ai/langgraph-tutorial-with-example/\", \"content\": \"Nodes are defined as Python functions that can perform a set of actions. For example, a node can integrate with a large language model, process information, call an external API, or any other task.\\\\n\\\\nA LangGraph node takes the state of the graph as a parameter and returns an updated state after it is executed.\\\\n\\\\nHere\\'s an example of a basic node, a Python function:\\\\n\\\\n```\\\\nfrom langchain_openai import ChatOpenAI\\\\n\\\\n...\\\\n\\\\nllm = ChatOpenAI(model_name=\\\\\"gpt-4o\\\\\")\", \"score\": 0.898277}]', name='tavily_search_results_json', tool_call_id='foo', artifact={'query': \"What's a 'node' in LangGraph?\", 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph', 'title': 'What is LangGraph?', 'content': 'Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as “actors” that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.', 'score': 0.9257218, 'raw_content': None}, {'url': 'https://www.gettingstarted.ai/langgraph-tutorial-with-example/', 'title': 'LangGraph Tutorial with Practical Example', 'content': 'Nodes are defined as Python functions that can perform a set of actions. For example, a node can integrate with a large language model, process information, call an external API, or any other task.\\n\\nA LangGraph node takes the state of the graph as a parameter and returns an updated state after it is executed.\\n\\nHere\\'s an example of a basic node, a Python function:\\n\\n```\\nfrom langchain_openai import ChatOpenAI\\n\\n...\\n\\nllm = ChatOpenAI(model_name=\"gpt-4o\")', 'score': 0.898277, 'raw_content': None}], 'response_time': 1.42, 'request_id': 'e5676238-69f2-42b4-93bd-86b3616b05e7'})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# results에 들어 있는 정보\n",
        "invoke_with_toolcall.content"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "6atM3Y3kHXoR",
        "outputId": "9c9e3817-5404-47cb-a1bc-692aae33e3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[{\"title\": \"What is LangGraph?\", \"url\": \"https://www.ibm.com/think/topics/langgraph\", \"content\": \"Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as “actors” that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.\", \"score\": 0.9257218}, {\"title\": \"LangGraph Tutorial with Practical Example\", \"url\": \"https://www.gettingstarted.ai/langgraph-tutorial-with-example/\", \"content\": \"Nodes are defined as Python functions that can perform a set of actions. For example, a node can integrate with a large language model, process information, call an external API, or any other task.\\\\n\\\\nA LangGraph node takes the state of the graph as a parameter and returns an updated state after it is executed.\\\\n\\\\nHere\\'s an example of a basic node, a Python function:\\\\n\\\\n```\\\\nfrom langchain_openai import ChatOpenAI\\\\n\\\\n...\\\\n\\\\nllm = ChatOpenAI(model_name=\\\\\"gpt-4o\\\\\")\", \"score\": 0.898277}]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델의 모든 실행결과\n",
        "invoke_with_toolcall.artifact"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02CuU2ifHZdW",
        "outputId": "451879a0-f73e-4ef3-ff24-9519b468767f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': \"What's a 'node' in LangGraph?\",\n",
              " 'follow_up_questions': None,\n",
              " 'answer': None,\n",
              " 'images': [],\n",
              " 'results': [{'url': 'https://www.ibm.com/think/topics/langgraph',\n",
              "   'title': 'What is LangGraph?',\n",
              "   'content': 'Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. Nodes can be thought of as “actors” that interact with each other in a specific way. For example, to add nodes for tool calling, one can use the ToolNode. Another example, the next node, refers to the node that will be executed following the current one.',\n",
              "   'score': 0.9257218,\n",
              "   'raw_content': None},\n",
              "  {'url': 'https://www.gettingstarted.ai/langgraph-tutorial-with-example/',\n",
              "   'title': 'LangGraph Tutorial with Practical Example',\n",
              "   'content': 'Nodes are defined as Python functions that can perform a set of actions. For example, a node can integrate with a large language model, process information, call an external API, or any other task.\\n\\nA LangGraph node takes the state of the graph as a parameter and returns an updated state after it is executed.\\n\\nHere\\'s an example of a basic node, a Python function:\\n\\n```\\nfrom langchain_openai import ChatOpenAI\\n\\n...\\n\\nllm = ChatOpenAI(model_name=\"gpt-4o\")',\n",
              "   'score': 0.898277,\n",
              "   'raw_content': None}],\n",
              " 'response_time': 1.42,\n",
              " 'request_id': 'e5676238-69f2-42b4-93bd-86b3616b05e7'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 도구 바인딩"
      ],
      "metadata": {
        "id": "8xfOi64cLnIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core"
      ],
      "metadata": {
        "id": "pId-0K91MgQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "Krsm-7sdLt4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데코레이터\n",
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    # 툴에 대한 설명이 없으면 에러난다.\n",
        "    \"\"\"Adds a and b\n",
        "\n",
        "    Arg:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply a and b\n",
        "\n",
        "    Arg:\n",
        "        a: first int\n",
        "        b: second int\n",
        "    \"\"\"\n",
        "    return a * b\n",
        "\n",
        "tools = [add, multiply]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "feg3wMLDLpIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1ZPqObYBLz7x",
        "outputId": "0169c1b8-fbbf-46db-bb2c-50dff876264a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.32-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.74 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.3.75)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (1.106.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.11.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.99.9->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.99.9->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.74->langchain_openai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.5.0)\n",
            "Downloading langchain_openai-0.3.32-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain_openai\n",
            "Successfully installed langchain_openai-0.3.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-5-nano\") # model=\"gpt-4o\"\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "bcSKUngQNzJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
        "\n",
        "# gpt모델은 tool이 필요없어서 tool을 호출하지않음\n",
        "llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kjKP-a3N1Km",
        "outputId": "824c4a24-c693-4df4-dd23-7df4488b7413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'multiply',\n",
              "  'args': {'a': 3, 'b': 12},\n",
              "  'id': 'call_AZPwCyv4OGlwdtyIxJsdB8xu',\n",
              "  'type': 'tool_call'},\n",
              " {'name': 'add',\n",
              "  'args': {'a': 11, 'b': 49},\n",
              "  'id': 'call_sU88vB5zRqdwyfBba41bprpp',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 12 % 2?\"\n",
        "\n",
        "llm_with_tools.invoke(query).tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHBB8MvIOulA",
        "outputId": "31996f27-15e3-4293-eb3a-f6a5e79d9026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools = llm.bind_tools(tools,\n",
        "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"multiply\"}}\n",
        ")\n",
        "\n",
        "resp = llm_with_tools.invoke(\"What is 3 * 12? Use tool.\")\n",
        "print(resp.tool_calls)  # 이제 비어있지 않음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6WdRY5QQAJy",
        "outputId": "4df77c04-0275-45c0-b3f4-45b38051036e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'multiply', 'args': {'a': 3, 'b': 12}, 'id': 'call_PF9QFtOiW9ybRxsEH0VRGS6P', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
        "llm_with_tools = llm.bind_tools(tools) # TavilySearch(tools) 을 호출할 수 있도록 함"
      ],
      "metadata": {
        "id": "YuwdY-CaQDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(\"안녕\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvt6OK-8QItd",
        "outputId": "2db8e580-ae61-4ca5-f9b6-e73dcc37723e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='안녕하세요! 무엇을 도와드릴까요? 정보 검색, 번역, 글쓰기, 문제 풀이, 아이디어 뽑기 등 다양한 도움 드릴 수 있어요. 원하시는 주제나 형식을 알려주시면 바로 도와드리겠습니다. 편하게 말씀해 주세요.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 328, 'prompt_tokens': 1351, 'total_tokens': 1679, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CE3z2UrzPJglQSkk2e42swvBh4MLk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2f79e286-9043-4355-bebf-bb0b9f350d82-0', usage_metadata={'input_tokens': 1351, 'output_tokens': 328, 'total_tokens': 1679, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 256}})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(\"What is Langgraph?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZmnBjBaQKbT",
        "outputId": "a20446ac-35c6-4bae-fdc9-217e1072b922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9LCIkIz4ZIHMJ5v1c7SOh1jQ', 'function': {'arguments': '{\"query\":\"Langgraph what is Langgraph\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 1354, 'total_tokens': 1448, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1152}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CE3z9si6uF7IQexE9sRydbEbdYGKK', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--a463717e-7db1-449c-ba3e-3387c81b0928-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'Langgraph what is Langgraph'}, 'id': 'call_9LCIkIz4ZIHMJ5v1c7SOh1jQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1354, 'output_tokens': 94, 'total_tokens': 1448, 'input_token_details': {'audio': 0, 'cache_read': 1152}, 'output_token_details': {'audio': 0, 'reasoning': 64}})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_with_tools.invoke(\"What is Langgraph?\").tool_calls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3v0rdUxQLzz",
        "outputId": "6b15c076-4216-44c1-cde1-9fee6b57eba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'tavily_search',\n",
              "  'args': {'query': 'Langgraph'},\n",
              "  'id': 'call_uOxsU8BqNsufmT3GeDks9QOq',\n",
              "  'type': 'tool_call'}]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oAvNkqC_QOMi",
        "outputId": "4cbe546d-4517-4217-85f9-085a1189352b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.75)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.6-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.6-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.6 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "SzLn1LUCQVrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]} # 일반적인 질문에 대한 일반 답변 or tool_calls\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JSRa_mYQb75",
        "outputId": "c256cb99-e1d0-4c88-cbe7-4322aba81bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7bf32eda3740>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. ToolNode\n",
        "ToolNode는 LangGraph에서 도구 호출을 실제로 실행해 주는 노드입니다. LLM이 생성한 AIMessage.tool_calls를 읽어 각 호출의 도구 이름과 인자를 매칭해 실행하고, 결과를 ToolMessage로 반환하여 그래프의 상태(대화 기록)에 추가합니다. 보통 “LLM 노드 → ToolNode → LLM 노드” 형태의 루프에서 사용되며, tools_condition 같은 조건부 엣지와 함께 붙여 LLM이 도구를 요청할 때만 ToolNode가 동작하게 합니다. 요약하면, ToolNode는 LLM의 툴 호출 계획을 실제 코드 실행으로 연결하는 브리지로, 도구 레지스트리(이름→함수/툴)만 넘겨주면 호출·에러 처리·결과 전달까지 표준화된 방식으로 처리해줍니다."
      ],
      "metadata": {
        "id": "o7V3mZSvQ9bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "class BasicToolNode:\n",
        "    def __init__(self, tools: list) -> None:\n",
        "        self.tools_by_name = {tool.name: tool for tool in tools} # [\"tavily_search\" : TavilySearch()]\n",
        "\n",
        "    def __call__(self, inputs: dict):\n",
        "        if messages := inputs.get(\"messages\", []):\n",
        "            message = messages[-1] # 마지막 message\n",
        "        else:\n",
        "            raise ValueError(\"No message found in input\")\n",
        "\n",
        "        outputs = []\n",
        "        for tool_call in message.tool_calls: # 메시지에서 호출된 도구를 불러옴\n",
        "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke( # Tool 호출 실행\n",
        "                tool_call[\"args\"]\n",
        "            )\n",
        "            outputs.append( # Tool 호출 결과(ToolMessage) 추가\n",
        "                ToolMessage(\n",
        "                    content=json.dumps(tool_result),\n",
        "                    name=tool_call[\"name\"],\n",
        "                    tool_call_id=tool_call[\"id\"],\n",
        "                )\n",
        "            )\n",
        "        return {\"messages\": outputs}\n",
        "\n",
        "tool_node = BasicToolNode(tools=[tool])\n",
        "# tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crelUazDRAAT",
        "outputId": "3fb17722-9813-4fa2-b5f0-cd0480d9fdb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7bf32eda3740>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def route_tools(\n",
        "    state: State,\n",
        "):\n",
        "\n",
        "    if isinstance(state, list):\n",
        "        ai_message = state[-1]\n",
        "    elif messages := state.get(\"messages\", []):\n",
        "        ai_message = messages[-1]\n",
        "    else:\n",
        "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "\n",
        "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_tools,\n",
        "    {\"tools\": \"tools\", END: END},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciLGAeG5RCjm",
        "outputId": "333d6ca9-6c2f-4f34-c826-e23eacbd4983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7bf32eda3740>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 엣지 연결\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\") # 도구가 호출될 때마다 챗봇으로 돌아가 다음 단계를 결정\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "6C_2K82LREFd",
        "outputId": "7a6ba75a-8bb2-4fee-dcb5-a8eceeeac360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7bf32e34ddf0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def stream_graph_updates(user_input: str):\n",
        "    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}): # graph 노드 호출 결과 받아옴\n",
        "        for value in event.values():\n",
        "            print(\"Assistant:\", value[\"messages\"][-1].content) # AI 답변 출력"
      ],
      "metadata": {
        "id": "FF8kxHq-Ski6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        stream_graph_updates(user_input)\n",
        "    except:\n",
        "        user_input = \"What do you know about LangGraph?\"\n",
        "        print(\"User: \" + user_input)\n",
        "        stream_graph_updates(user_input)\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z7DOjSLXSk-M",
        "outputId": "5b02b7d8-7d38-4113-d3b2-694b05998038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: 하이\n",
            "User: 하이\n",
            "Assistant: 안녕하세요! 무엇을 도와드릴까요? 필요하신 분야를 말씀해 주시면 바로 도와드리겠습니다. 예시로는\n",
            "\n",
            "- 질문에 답하기\n",
            "- 글쓰기 아이디어 제공/수정\n",
            "- 요약/정리\n",
            "- 번역\n",
            "- 간단한 계산이나 정보 검색\n",
            "- 코딩 도움\n",
            "\n",
            "중 편하신 걸로 시작해 볼까요?\n",
            "User: 이력서 좀 써줘\n",
            "User: 이력서 좀 써줘\n",
            "Assistant: 좋아요! 이력서를 맞춤형으로 작성해 드리려면 몇 가지 정보를 알려주시면 됩니다. 먼저 기본 템플릿을 드리고, 필요한 내용을 채워 주시면 바로 완성본으로 다듬어 드릴게요. \n",
            "\n",
            "다양한 형식 중 선택 가능\n",
            "- 한국식 이력서 (1장 중심, 간결하게 핵심 성과 위주)\n",
            "- 다국적 기업용 영어/영문 이력서\n",
            "- 이력서와 간단한 자기소개서가 함께 필요하면 같이 구성\n",
            "\n",
            "먼저 채워주시면 좋을 정보\n",
            "- 지원 직무/업종: 어떤 직무에 지원하나요? 예: 마케팅 매니저, 소프트웨어 엔지니어 등\n",
            "- 최종 학력: 학교 이름, 학위, 전공, 졸업 연도\n",
            "- 경력 요약: 최근 직무 한두 줄 요약\n",
            "- 주요 경력 항목(최근 3개 정도): 회사명, 직책, 재직 기간, 핵심 업무 3–5개와 성과(숫자 포함 가능하면 더 좋습니다)\n",
            "- 핵심 기술/역량: 프로그래밍 언어, 도구, 소프트웨어, 방법론 등\n",
            "- 자격증/수상 내역: 자격증 이름, 발급기관, 취득연도\n",
            "- 언어 능력: 외국어 및 숙련도\n",
            "- 프로젝트/포트폴리오: 중요 프로젝트 이름, 역할, 사용 기술, 성과\n",
            "- 기타: 어필하고 싶은 강점이나 수치화된 성과가 있다면\n",
            "- 원하시는 이력서 길이: 1장/2장\n",
            "- 포맷 언어: 한국어 버전, 영어 버전, 또는 이중 버전 여부\n",
            "- 링크/포트폴리오: LinkedIn, GitHub, 홈페이지 등 필요하면 URL\n",
            "\n",
            "제안하는 기본 템플릿(한국식, 1장)\n",
            "- 개인 정보: 이름, 연락처, 이메일, 주소(선택), LinkedIn\n",
            "- 채용 목표 프로필(2–3문장): 지원 직무에 맞춘 핵심 역량과 가치 제시\n",
            "- 핵심 역량(스킬 요약): 6–8개 핵심 기술\n",
            "- 경력 사항(최근 순으로 3–4개 항목): 회사명, 직책, 기간, 3–5개 핵심 성과를 수치로\n",
            "  예) “매출 X% 증가를 견인한 Y 캠페인 기획 및 실행” \n",
            "- 학력\n",
            "- 자격증/수상\n",
            "- 프로젝트/포트폴리오(선택적): 간단한 요약 및 사용 기술\n",
            "- 어학/기타 활동\n",
            "- 추천인(선택적)\n",
            "\n",
            "예시 문장 스타일(성과 중심의 bullet 포맷)\n",
            "- 책임/업무 요약 + 구체적 성과 + 수치\n",
            "- 예1: “A 시스템 도입으로 처리 시간 40% 단축, 월간 비용 20% 절감”\n",
            "- 예2: “크로스-펑션 팀과 협업하여 신규 기능 3개 론칭, 사용자 만족도 NPS 25포인트 상승”\n",
            "- 예3: “자동화 도구 도입으로 QA 사이클 시간 60% 감소”\n",
            "\n",
            "다음 메시지로 주시면 바로 초안 형태로 작성해 드립니다\n",
            "- 지원 직무/업종\n",
            "- 최종 학력(학교, 학위, 전공, 연도)\n",
            "- 최근 경력 1~2곳의 정보(회사명, 직책, 근무기간, 주요 성과)\n",
            "- 핵심 기술/자격증\n",
            "- 언어 능력\n",
            "- 포트폴리오 링크가 있으면 공유\n",
            "\n",
            "원하시면 바로 제가 내용을 받아 한국식 1장 이력서와 영어 버전(또는 이중 버전) 초안을 만들어 드릴게요.\n",
            "User: What do you know about LangGraph?\n",
            "Assistant: \n",
            "Assistant: {\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent\\u2019s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.93666285, \"raw_content\": null}, {\"url\": \"https://langchain-ai.github.io/langgraphjs/\", \"title\": \"LangGraph.js\", \"content\": \"LangGraph \\u2014 used by Replit, Uber, LinkedIn, GitLab and more \\u2014 is a low-level orchestration framework for building controllable agents\", \"score\": 0.8925721, \"raw_content\": null}], \"response_time\": 1.56, \"request_id\": \"2efd7a39-3784-4b84-b7da-8563f68acee7\"}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1686035356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What do you know about LangGraph?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mstream_graph_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-752204051.py\u001b[0m in \u001b[0;36mstream_graph_updates\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstream_graph_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# graph 노드 호출 결과 받아옴\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Assistant:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# AI 답변 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2645\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2648\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    163\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2667419402.py\u001b[0m in \u001b[0;36mchatbot\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mllm_with_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# 일반적인 질문에 대한 일반 답변 or tool_calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgraph_builder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"chatbot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchatbot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5493\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5494\u001b[0m     ) -> Output:\n\u001b[0;32m-> 5495\u001b[0;31m         return self.bound.invoke(\n\u001b[0m\u001b[1;32m   5496\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_configs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         return cast(\n\u001b[1;32m    392\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    394\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1018\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m                 results.append(\n\u001b[0;32m--> 837\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    838\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 )\n\u001b[1;32m   1177\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1146\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m         )\n\u001b[0;32m-> 1259\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 response = self._client.send(\n\u001b[0m\u001b[1;32m    983\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                     \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. create_react_agent\n",
        "create_react_agent는 LangChain에서 ReAct 패턴(Reason+Act)을 따르는 에이전트를 손쉽게 구성하는 팩토리로, LLM과 사용할 도구 목록, 그리고 적절한 프롬프트를 결합해 “생각→도구 호출→관찰→최종 답변”의 반복 루프를 수행하는 Agent 객체를 만들어줍니다. 이 에이전트는 질문을 해석해 필요한 도구를 선택하고 인자를 구성해 호출한 뒤, 결과를 반영해 다음 행동을 결정하며, 보통 AgentExecutor와 함께 실행하여 다단계 추론과 복수의 툴 호출을 자동으로 오케스트레이션합니다. 핵심은 프롬프트(지침), LLM, 툴 레지스트리(이름→함수/API), 출력 파서를 표준화해 붙여주는 것이며, OpenAI 스타일의 툴콜을 포함한 다양한 LLM과 호환되어 실용적인 “생각하며 도구를 쓰는” 에이전트를 빠르게 구성할 수 있게 해주는 점입니다."
      ],
      "metadata": {
        "id": "QjSFLATaS-IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "tool = TavilySearch(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-5-nano\")\n",
        "agent = create_react_agent(llm, tools)"
      ],
      "metadata": {
        "id": "2cO9_kqUTAf6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is LangGraph?\"}]})"
      ],
      "metadata": {
        "id": "eAgykSV9TCTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLdPIUQdULQS",
        "outputId": "dcc7c964-61fb-404a-8372-9860366a9be1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}, id='9707d3d5-9bd5-4e82-9041-2650e0e284be'),\n",
              "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Qtd8VQiSZKsfCx9y6MLbcoxe', 'function': {'arguments': '{\"query\":\"LangGraph\"}', 'name': 'tavily_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 1354, 'total_tokens': 1508, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CE4BNj8iTJAJIcbaImyra0XXXx0cJ', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8fe9a1d0-30a3-4532-ab5f-4f951e3bf040-0', tool_calls=[{'name': 'tavily_search', 'args': {'query': 'LangGraph'}, 'id': 'call_Qtd8VQiSZKsfCx9y6MLbcoxe', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1354, 'output_tokens': 154, 'total_tokens': 1508, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
              "  ToolMessage(content='{\"query\": \"LangGraph\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.ibm.com/think/topics/langgraph\", \"title\": \"What is LangGraph? - IBM\", \"content\": \"LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. **Nodes**: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.\", \"score\": 0.93666285, \"raw_content\": null}, {\"url\": \"https://langchain-ai.github.io/langgraphjs/\", \"title\": \"LangGraph.js\", \"content\": \"LangGraph — used by Replit, Uber, LinkedIn, GitLab and more — is a low-level orchestration framework for building controllable agents\", \"score\": 0.8925721, \"raw_content\": null}], \"response_time\": 1.69, \"request_id\": \"1175f4e1-46ba-4d5b-82d9-8b88b307a98b\"}', name='tavily_search', id='bdd81b7f-ce62-4e9c-9a7f-db320e443fba', tool_call_id='call_Qtd8VQiSZKsfCx9y6MLbcoxe'),\n",
              "  AIMessage(content='LangGraph can refer to a couple of related things in the AI agent ecosystem, mostly around graph-based orchestration of agents. There are two common interpretations:\\n\\n- LangGraph (LangChain ecosystem): An open-source framework for building, deploying, and managing complex AI agent workflows using a graph-based architecture. In this view, a workflow is a graph where nodes are components (agents, tools, prompts, memory, etc.) and edges define the flow of data and decisions. The goal is to improve transparency, debuggability, and control over multi-step AI tasks.\\n\\n- LangGraph.js: A low-level JavaScript/TypeScript library for orchestrating controllable agents. It provides building blocks to compose and manage agent behavior at a fine-grained level and is used by large organizations to run complex agent workflows.\\n\\nIf you tell me which one you’re after (the LangGraph framework in the LangChain ecosystem or the LangGraph.js library), I can give you more detailed info, examples, or implementation guidance.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1619, 'prompt_tokens': 1738, 'total_tokens': 3357, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1408, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CE4BQcuKQP7SVPMn6U1UJ7eKBSW9p', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--fe09babb-dbe3-4b4d-b755-319847240644-0', usage_metadata={'input_tokens': 1738, 'output_tokens': 1619, 'total_tokens': 3357, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1408}})]}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 원하는 형태로 출력하는 챗봇"
      ],
      "metadata": {
        "id": "T73L4HTMYJx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "rdn1DhgIYLvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieResponse(BaseModel):\n",
        "    title: str = Field(description = '영화제목')\n",
        "    director :  str = Field(description = '감독이름')\n",
        "    genre :  str = Field(description = '장르')\n",
        "    release_year :  int = Field(description = '개봉연도')"
      ],
      "metadata": {
        "id": "Spu9LXmGYPbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
        "model_with_structured_output = model.with_structured_output(MovieResponse)\n",
        "\n",
        "model_with_structured_output.invoke('메멘토 영화에 대해 설명해주세요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exoXa9f5Y-hw",
        "outputId": "56b505ca-a877-41d7-bf7a-77986a8aec77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MovieResponse(title='메멘토', director='크리스토퍼 놀런', genre='심리 스릴러, 미스터리', release_year=2000)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Union"
      ],
      "metadata": {
        "id": "wgEXr5o_Zc0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MovieResponse(BaseModel):\n",
        "    title: str = Field(description = '영화제목')\n",
        "    director :  str = Field(description = '감독이름')\n",
        "    genre :  str = Field(description = '장르')\n",
        "    release_year :  int = Field(description = '개봉연도')\n",
        "\n",
        "class ConversationalResponse(BaseModel):\n",
        "    response: str = Field(description = '사용자의 질문에 대해 친절하게 대화하듯 답변하는 문장')\n",
        "\n",
        "class FinalResponse(BaseModel):\n",
        "    final_output:Union[MovieResponse, ConversationalResponse]"
      ],
      "metadata": {
        "id": "Nh4XCoMwZx0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm = model.with_structured_output(FinalResponse)\n",
        "structured_llm.invoke('메멘토 영화에 대해 설명해주세요')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMlJzBTEac0U",
        "outputId": "2381a0c1-9925-4bea-a67a-f3fc2b86f1ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalResponse(final_output=ConversationalResponse(response='메멘토는 크리스토퍼 놀란 감독의 2000년작 네오 누아르 심리 스릴러로, 기억과 정체성을 주제로 한 비선형 서사가 큰 특징입니다. 주인공 레너드 셸비(가이 피어스)는 아내를 잃은 뒤 단기 기억상실을 겪고 있어, 15~20분 정도의 기억만 유지됩니다. 그는 아내의 살해를 복수하기 위해 사건의 단서를 쫓지만, 몸에 남긴 타투와 사진, 메모에 의존해 앞으로 나아가야 하므로 진실의 안정성은 끊임없이 흔들립니다. 영화의 색 화면은 현재의 사건을 역순으로 보여주고 흑백 화면은 시간의 흐름대로 진행되어 두 서사가 서로 얽히며 관객의 기억에 대한 신뢰를 계속 의심하게 만듭니다. 이로써 기억의 왜곡, 진실과 허구의 경계, 그리고 복수의 윤리에 관한 질문들을 던지는 작품입니다.\\n\\n참고 정보: 감독 - 크리스토퍼 놀란, 배우 - 가이 피어스, 캐리 애너 모스, 조 패토리노. 장르 - 네오 누아르/심리 스릴러. 런타임 약 113분. 예산은 비교적 낮은 편이었지만 전 세계적으로 큰 화제를 모았고, 영화적 구조와 주제 때문에 지금까지도 다수의 영화 애호가와 학자들에게 자주 분석됩니다.\\n\\n스포일러를 원하시면 줄거리의 반전과 결말까지 자세히 설명해 드리겠습니다. 먼저 스포일러 없는 요약으로 충분하신지 알려 주세요.'))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_llm.invoke('오늘 점심 뭐먹지?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpoPR7VVbM70",
        "outputId": "a83c4b6c-de89-4602-ad21-73bb63d2e209"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FinalResponse(final_output=ConversationalResponse(response='오늘 점심으로 몇 가지 아이디어를 드릴게요. 원하시는 분위기나 제약이 있다면 알려주시면 바로 맞춤 추천해드려요.\\n\\n1) 가볍고 빠르게: 샌드위치나 김밥, 커피 한 잔 합치면 10–15분 내외로 해결\\n2) 든든하게: 불고기덮밥, 제육덮밥, 비빔밥처럼 한 그릇 요리\\n3) 건강하게: 그린 샐러드 볼에 구운 닭가슴살이나 연어 추가\\n4) 면 요리: 짜장면/짬뽕/냉면 중 선택\\n5) 따뜻한 국물: 된장찌개나 순두부찌개와 밥\\n\\n원하시면 근처 맛집이나 배달 메뉴도 바로 추천해드릴게요. 예산대나 피하고 싶은 재료(예: 해산물, 매운맛)도 알려주면 더 정확히 맞춰드려요.'))"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "from langgraph.graph import MessagesState\n",
        "from typing import Literal"
      ],
      "metadata": {
        "id": "ilOZ34xIbSJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(MessagesState):\n",
        "    final_response:MovieResponse"
      ],
      "metadata": {
        "id": "1PfnkMxvbojD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 내 메세지에 따라 분기하는 코드\n",
        "@tool\n",
        "def get_movieinfo(movie:Literal['메멘토','인터스텔라']):\n",
        "    \"\"\" 아래 설명은 영화에 대한 내용이야. 참고해줘 \"\"\"\n",
        "    if movie == \"메멘토\":\n",
        "        return \"메멘토는 단기 기억 상실증을 가진 주인공이 아내 살해 사건의 진실을 찾기 위해 메모와 문신에 의존해 사건을 추적하는 독특한 구조의 스릴러 영화입니다. \"\n",
        "    elif movie == \"인터스텔라\":\n",
        "        return \"인터스텔라는 인류의 미래를 구하기 위해 우주로 떠난 탐사대가 사랑과 시간, 과학의 한계를 넘어서며 펼치는 감동적인 SF 영화입니다.\"\n",
        "    else:\n",
        "        raise AssertionError(\"알수없는 영화\")"
      ],
      "metadata": {
        "id": "AUv1GmiKb5Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [get_movieinfo] # tool로 등록\n",
        "model_with_tool = model.bind_tools(tools)"
      ],
      "metadata": {
        "id": "PcBlR1xacnfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call_model(state:State):\n",
        "    response = model_with_tool.invoke(state['messages'])\n",
        "    return {'messages': [response]}"
      ],
      "metadata": {
        "id": "q-CnwIkXdgJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "v7RT9Ggjd2VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model=\"gpt-5-nano\")\n",
        "model_with_structured_output = model.with_structured_output(MovieResponse)"
      ],
      "metadata": {
        "id": "jNNHAEAHd6_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def respond(state:State):\n",
        "    # [-1] : AIMessage , [-2]: ToolMessage\n",
        "    response = model_with_structured_output.invoke(\n",
        "        [HumanMessage(content=state['messages'][-2].content)]\n",
        "    )\n",
        "    return {'final_response': response}"
      ],
      "metadata": {
        "id": "1BTHZjspeMup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def should_continue(state:State):\n",
        "    messages = state['messages']\n",
        "    last_message = messages[-1]\n",
        "    if not last_message.tool_calls:\n",
        "        return \"respond\"\n",
        "    else:\n",
        "        return \"continue\""
      ],
      "metadata": {
        "id": "xY0Rx4Bueq20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "graph_builder.add_node(\"agent\", call_model)\n",
        "graph_builder.add_node(\"respond\", respond)\n",
        "graph_builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "graph_builder.set_entry_point(\"agent\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\":\"tools\",\n",
        "        \"respond\":\"respond\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_builder.add_edge(\"tools\",\"agent\")\n",
        "graph_builder.add_edge(\"respond\",END)\n",
        "graph = graph_builder.compile()\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "vdt0yT3VfCja",
        "outputId": "c4c51a6c-4c3e-4454-e136-eb52204bada9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.CompiledStateGraph object at 0x7bf3270267b0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"메멘토 영화에 대해 알려주세요\")]})[\n",
        "    \"final_response\"\n",
        "]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkWteRKHf77K",
        "outputId": "d701326b-6879-4000-d741-f68c4040c13a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MovieResponse(title='Memento', director='Christopher Nolan', genre='스릴러, 심리 스릴러, 미스터리', release_year=2000)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = graph.invoke(input={\"messages\": [(\"human\", \"인터스텔라 영화에 대해 알려주세요\")]})[\n",
        "    \"final_response\"\n",
        "]\n",
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmDdsSPGgrPo",
        "outputId": "d4965640-9dbf-4de7-c3dc-6a123067dec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MovieResponse(title='인터스텔라', director='크리스토퍼 놀란', genre='SF, 드라마', release_year=2014)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 과제\n",
        "\n",
        "<img src=\"https://blog.kakaocdn.net/dna/dvE95o/btsQsGv9Ped/AAAAAAAAAAAAAAAAAAAAAEr7qqddi3MvQ-cZQuNLgsrubcxLHGFIM-LiZYy4PcZF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1759244399&allow_ip=&allow_referer=&signature=%2FDzXWDuyjHqGL%2B4EchhibHrMt7I%3D\">\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IPG4G7KdjQxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import ArxivAPIWrapper"
      ],
      "metadata": {
        "id": "_EfBsDhdizjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-7B44EVjZin",
        "outputId": "fab0dd27-b0cb-418d-afca-ddd86e2f7604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.12/dist-packages (from arxiv) (2.32.5)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.0->arxiv) (2025.8.3)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=f68fdb7dcf6d549869235089ced0e2c927cdb3e52e83fc42e0f7762c04e74688\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.2.0 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arxiv = ArxivAPIWrapper()\n",
        "docs = arxiv.run(\"1706.03762\")"
      ],
      "metadata": {
        "id": "HnPxYrJni_Nz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}