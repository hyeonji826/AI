{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+OZvHGQfuolKjBIEP2YqQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. 선형 회귀 분석\n","선형 회귀 분석(Linear Regression)은 주어진 데이터에서 입력 변수(독립 변수)와 출력 변수(종속 변수) 사이의 관계를 직선(또는 다차원에서는 평면)으로 설명하고, 새로운 입력 값에 대한 출력을 예측하는 통계 및 머신러닝 기법입니다. 예를 들어, 공부 시간(입력 변수)과 시험 점수(출력 변수) 사이의 관계를 분석해 \"공부 시간이 늘어날수록 시험 점수가 증가한다\"는 패턴을 찾아냅니다. 이 과정에서 선형 회귀는 \"Y = W X + b\"라는 수식(기울기 W와 절편 b)으로 데이터를 표현하며, 최적의 기울기와 절편을 찾기 위해 비용 함수(Cost Function)를 최소화하는 경사 하강법(Gradient Descent) 등의 알고리즘을 사용합니다. 최종적으로 선형 회귀 모델은 주어진 입력 값에 대해 가장 적합한 예측 결과를 제공합니다.\n","\n","> **경사하강법**\n","- 비용 함수의 값을 최소로 하는 W와 b를 찾는 알고리즘을 옵티마이저 알고리즘이라고 함\n","- 최적화 알고리즘이라고도 함\n","- 옵티마이저 알고리즘을 통해 W와 b를 찾아내는 과정을 '학습'이라고 부름\n","- 경사 하강법은 가장 기본적인 옵티마이저 알고리즘\n","\n","- 출처: https://coding-yesung.tistory.com/214 [코딩하는 춘식이:티스토리]\n","<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdna%2FbyRI4b%2FbtsludLaVjV%2FAAAAAAAAAAAAAAAAAAAAACvsfGnsCTjW0CYZ5yH-HAfSeJ27bj4BJkkBfPlHVpCN%2Fimg.png%3Fcredential%3DyqXZFxpELC7KVnFOS48ylbz2pIh7yKj8%26expires%3D1753973999%26allow_ip%3D%26allow_referer%3D%26signature%3DfSwZGVCKIjSj67HX8ioUvembs74%253D\">"],"metadata":{"id":"1dghxV7f71fj"}},{"cell_type":"markdown","source":["## 2. 단항 선형 회귀\n","단항 선형 회귀(Simple Linear Regression)는 하나의 독립 변수(입력 변수, X)를 사용하여 하나의 종속 변수(출력 변수, Y)를 예측하는 통계 및 머신러닝 기법입니다. 입력 변수와 출력 변수 사이의 관계를 직선(Linear Line)으로 나타내며, 데이터의 패턴을 기반으로 가장 잘 맞는 직선을 찾아내어 새로운 입력 값에 대한 출력을 예측합니다."],"metadata":{"id":"aZPwNKCR765Y"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"sfYJ_GZ_7o1T","executionInfo":{"status":"ok","timestamp":1753407858418,"user_tz":-540,"elapsed":8388,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}}},"outputs":[],"source":["import torch\n","# 파이토치를 이용해서 ML/DL모델을 만들 수 있도록 해주는 모듈\n","import torch.nn as nn\n","# 옵티마이저 알고리즘(최적화 or 경사하강법 알고리즘)\n","import torch.optim as optim\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["torch.manual_seed(2025)\n","# 일단 랜덤하게 선이 그어지는데 그 랜덤하게 나오는 고정값을 설정함 -> 2025 로 고정"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9oJ6ZeA9vxD","executionInfo":{"status":"ok","timestamp":1753407858426,"user_tz":-540,"elapsed":69,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"0bbd0ab3-1ae3-481f-8a85-cd42aa562703"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x78c3ad7be790>"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["x_train = torch.FloatTensor([[1], [2], [3]])\n","y_train = torch.FloatTensor([[2], [4], [6]])\n","print(x_train,x_train.shape)\n","print(y_train,y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CiEYpWFs9yc1","executionInfo":{"status":"ok","timestamp":1753407858524,"user_tz":-540,"elapsed":110,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"61ed5041-5cba-4ccb-f20a-c6ec89fcad0a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.],\n","        [2.],\n","        [3.]]) torch.Size([3, 1])\n","tensor([[2.],\n","        [4.],\n","        [6.]]) torch.Size([3, 1])\n"]}]},{"cell_type":"code","source":["plt.figure(figsize=(6, 4))\n","plt.scatter(x_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386},"id":"Ic2i50mq-MXw","executionInfo":{"status":"ok","timestamp":1753407859147,"user_tz":-540,"elapsed":610,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"1c68f1f6-325f-4a48-b553-b50add0645e3"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.collections.PathCollection at 0x78c2c6fe1250>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAgQAAAFfCAYAAAAxo9Q/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAJ3xJREFUeJzt3X9w1PWdx/HXhpANp9klKMkGWNMgGAgh8ksgQQuWQECGIf2jYgYMtsCdTJiD3mlrHO8QmGuwSC2tTgAVYo/DHNgCJ0IwgomDCUUgTBNoOcBIgibhzsJugrLlsp/7g2F1Jb92k5AEno+Z78h+9v3dfX/mw7ovvvvd71qMMUYAAOCOFtLVDQAAgK5HIAAAAAQCAABAIAAAACIQAAAAEQgAAIAIBAAAQFJoVzfQFl6vV1988YUiIiJksVi6uh0AAHoMY4zq6+s1YMAAhYQ0fxygRwSCL774Qk6ns6vbAACgx6qurtagQYOavb9HBIKIiAhJ1ydjs9m6uBsAAHoOt9stp9Ppey9tTo8IBDc+JrDZbAQCAACC0NpH7pxUCAAACAQAAIBAAAAARCAAAAAiEAAAABEIAACAesjXDgEAuN01eo2OVP5VF+uvKioiXOPj+qlXyK27Oi+BAACALlZQUaOV755SjeuqbyzGHq4VsxM0IzHmlvQQ8EcGn3/+uebPn6977rlHffr00ciRI3X06NEW9ykqKtKYMWNktVo1ZMgQ5eXlBdsvAAC3lYKKGi3ZetwvDEhSreuqlmw9roKKmlvSR0CB4NKlS5o0aZJ69+6tffv26dSpU1q3bp0iIyOb3aeyslKzZs3So48+qhMnTmj58uVatGiR9u/f3+7mAQDoyRq9RivfPSXTxH03xla+e0qN3qYqOlZAHxm89NJLcjqd2rJli28sLi6uxX02bNiguLg4rVu3TpI0fPhwHTp0SK+88orS0tKa3Mfj8cjj8fhuu93uQNoEAKBHOFL515uODHybkVTjuqojlX9V8v33dGovAR0h+K//+i+NGzdOP/rRjxQVFaXRo0fr9ddfb3Gf0tJSpaam+o2lpaWptLS02X1ycnJkt9t9G790CAC4HV2sbz4MBFPXHgEFgk8//VS5ubkaOnSo9u/fryVLlugf//Ef9dZbbzW7T21traKjo/3GoqOj5Xa79fXXXze5T3Z2tlwul2+rrq4OpE0AAHqEqIjwDq1rj4A+MvB6vRo3bpx+8YtfSJJGjx6tiooKbdiwQQsWLOiwpqxWq6xWa4c9HgAA3dH4uH6KsYer1nW1yfMILJIc9utfQexsAR0hiImJUUJCgt/Y8OHDVVVV1ew+DodDdXV1fmN1dXWy2Wzq06dPIE8PAMBtpVeIRStmX39f/e4VB27cXjE74ZZcjyCgQDBp0iSdPn3ab+y///u/FRsb2+w+ycnJOnDggN9YYWGhkpOTA3lqAABuSzMSY5Q7f4wcdv+PBRz2cOXOH3PLrkMQ0EcGP/3pT5WSkqJf/OIXevzxx3XkyBFt2rRJmzZt8tVkZ2fr888/1+9+9ztJ0tNPP61XX31VP/vZz/STn/xEBw8e1Pbt2/Xee+917EwAAOihZiTGaFqCo+dcqfChhx7Szp07lZ2drVWrVikuLk6//vWvNW/ePF9NTU2N30cIcXFxeu+99/TTn/5U69ev16BBg/TGG280+5VDAADuRL1CLJ3+1cKWWIwxnX+1g3Zyu92y2+1yuVyy2Wxd3Q4AAD1GW99D+bVDAABAIAAAAAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAAAowELz44ouyWCx+27Bhw5qtz8vLu6k+PDy83U0DAICOFRroDiNGjNAHH3zwzQOEtvwQNptNp0+f9t22WCyBPiUAAOhkAQeC0NBQORyONtdbLJaA6gEAwK0X8DkEZ86c0YABAzR48GDNmzdPVVVVLdY3NDQoNjZWTqdTc+bM0cmTJ1t9Do/HI7fb7bcBAIDOE1AgmDBhgvLy8lRQUKDc3FxVVlbqkUceUX19fZP18fHx2rx5s3bv3q2tW7fK6/UqJSVFFy5caPF5cnJyZLfbfZvT6QykTQAAECCLMcYEu/Ply5cVGxurX/3qV1q4cGGr9deuXdPw4cOVkZGh1atXN1vn8Xjk8Xh8t91ut5xOp1wul2w2W7DtAgBwx3G73bLb7a2+hwZ8DsG39e3bVw888IDOnj3bpvrevXtr9OjRrdZbrVZZrdb2tAYAAALQrusQNDQ06Ny5c4qJiWlTfWNjo8rLy9tcDwAAbo2AAsEzzzyj4uJiffbZZyopKdEPf/hD9erVSxkZGZKkzMxMZWdn++pXrVql999/X59++qmOHz+u+fPn6/z581q0aFHHzgIAALRLQB8ZXLhwQRkZGfryyy/Vv39/Pfzwwzp8+LD69+8vSaqqqlJIyDcZ49KlS1q8eLFqa2sVGRmpsWPHqqSkRAkJCR07CwAA0C7tOqnwVmnrCREAAMBfW99D+S0DAABAIAAAAAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIACDAQvvviiLBaL3zZs2LAW99mxY4eGDRum8PBwjRw5Unv37m1XwwAAoOMFfIRgxIgRqqmp8W2HDh1qtrakpEQZGRlauHChysrKlJ6ervT0dFVUVLSraQAA0LFCA94hNFQOh6NNtevXr9eMGTP07LPPSpJWr16twsJCvfrqq9qwYUOz+3k8Hnk8Ht9tt9sdaJsAACAAAR8hOHPmjAYMGKDBgwdr3rx5qqqqara2tLRUqampfmNpaWkqLS1t8TlycnJkt9t9m9PpDLRNAAAQgIACwYQJE5SXl6eCggLl5uaqsrJSjzzyiOrr65usr62tVXR0tN9YdHS0amtrW3ye7OxsuVwu31ZdXR1ImwAAIEABfWQwc+ZM35+TkpI0YcIExcbGavv27Vq4cGGHNWW1WmW1Wjvs8QAAQMva9bXDvn376oEHHtDZs2ebvN/hcKiurs5vrK6urs3nIAAAgFujXYGgoaFB586dU0xMTJP3Jycn68CBA35jhYWFSk5Obs/TAgCADhZQIHjmmWdUXFyszz77TCUlJfrhD3+oXr16KSMjQ5KUmZmp7OxsX/2yZctUUFCgdevW6S9/+YtefPFFHT16VEuXLu3YWQAAgHYJ6ByCCxcuKCMjQ19++aX69++vhx9+WIcPH1b//v0lSVVVVQoJ+SZjpKSkaNu2bXrhhRf0/PPPa+jQodq1a5cSExM7dhYAAKBdLMYY09VNtMbtdstut8vlcslms3V1OwAA9BhtfQ/ltwwAAACBAAAAEAgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAqJ2BYM2aNbJYLFq+fHmzNXl5ebJYLH5beHh4e54WAAB0sNBgd/zkk0+0ceNGJSUltVprs9l0+vRp322LxRLs0wIAgE4Q1BGChoYGzZs3T6+//roiIyNbrbdYLHI4HL4tOjo6mKcFAACdJKhAkJWVpVmzZik1NbVN9Q0NDYqNjZXT6dScOXN08uTJFus9Ho/cbrffBgAAOk/AgSA/P1/Hjx9XTk5Om+rj4+O1efNm7d69W1u3bpXX61VKSoouXLjQ7D45OTmy2+2+zel0BtomAAAIgMUYY9paXF1drXHjxqmwsNB37sCUKVM0atQo/frXv27TY1y7dk3Dhw9XRkaGVq9e3WSNx+ORx+Px3Xa73XI6nXK5XLLZbG1tFwCAO57b7Zbdbm/1PTSgkwqPHTumixcvasyYMb6xxsZGffTRR3r11Vfl8XjUq1evFh+jd+/eGj16tM6ePdtsjdVqldVqDaQ1AADQDgEFgqlTp6q8vNxv7Mc//rGGDRumn//8562GAel6gCgvL9djjz0WWKcAAKDTBBQIIiIilJiY6Dd211136Z577vGNZ2ZmauDAgb5zDFatWqWJEydqyJAhunz5stauXavz589r0aJFHTQFAADQXkFfh6A5VVVVCgn55lzFS5cuafHixaqtrVVkZKTGjh2rkpISJSQkdPRTAwCAIAV0UmFXaesJEQAAwF9b30P5LQMAAEAgAAAABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAIhAAAAARCAAAgAgEAABABAIAACACAQAAEIEAAACIQAAAAEQgAAAAkkK7ugEAPVej1+hI5V91sf6qoiLCNT6un3qFWLq6LQBBIBAACEpBRY1WvntKNa6rvrEYe7hWzE7QjMSYLuwMQDDa9ZHBmjVrZLFYtHz58hbrduzYoWHDhik8PFwjR47U3r172/O0ALpYQUWNlmw97hcGJKnWdVVLth5XQUVNF3UGIFhBB4JPPvlEGzduVFJSUot1JSUlysjI0MKFC1VWVqb09HSlp6eroqIi2KcG0IUavUYr3z0l08R9N8ZWvntKjd6mKgB0V0EFgoaGBs2bN0+vv/66IiMjW6xdv369ZsyYoWeffVbDhw/X6tWrNWbMGL366qvN7uPxeOR2u/02AN3Dkcq/3nRk4NuMpBrXVR2p/OutawpAuwUVCLKysjRr1iylpqa2WltaWnpTXVpamkpLS5vdJycnR3a73bc5nc5g2gTQCS7WNx8GgqkD0D0EHAjy8/N1/Phx5eTktKm+trZW0dHRfmPR0dGqra1tdp/s7Gy5XC7fVl1dHWibADpJVER4h9YB6B4C+pZBdXW1li1bpsLCQoWHd96L3Wq1ymq1dtrjAwje+Lh+irGHq9Z1tcnzCCySHPbrX0EE0HMEdITg2LFjunjxosaMGaPQ0FCFhoaquLhYv/nNbxQaGqrGxsab9nE4HKqrq/Mbq6urk8PhaF/nALpErxCLVsxOkHT9zf/bbtxeMTuB6xEAPUxAgWDq1KkqLy/XiRMnfNu4ceM0b948nThxQr169bppn+TkZB04cMBvrLCwUMnJye3rHECXmZEYo9z5Y+Sw+x8pdNjDlTt/DNchAHqggD4yiIiIUGJiot/YXXfdpXvuucc3npmZqYEDB/rOMVi2bJkmT56sdevWadasWcrPz9fRo0e1adOmDpoCgK4wIzFG0xIcXKkQuE10+JUKq6qqFBLyzYGHlJQUbdu2TS+88IKef/55DR06VLt27bopWADoeXqFWJR8/z1d3QaADmAxxnT7q4e43W7Z7Xa5XC7ZbLaubgcAgB6jre+h/NohAAAgEAAAAAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAAAUYCHJzc5WUlCSbzSabzabk5GTt27ev2fq8vDxZLBa/LTw8vN1NAwCAjhUaSPGgQYO0Zs0aDR06VMYYvfXWW5ozZ47Kyso0YsSIJvex2Ww6ffq077bFYmlfxwAAoMMFFAhmz57td/vf/u3flJubq8OHDzcbCCwWixwOR/AdAgCAThf0OQSNjY3Kz8/XlStXlJyc3GxdQ0ODYmNj5XQ6NWfOHJ08ebLVx/Z4PHK73X4bAADoPAEHgvLyct19992yWq16+umntXPnTiUkJDRZGx8fr82bN2v37t3aunWrvF6vUlJSdOHChRafIycnR3a73bc5nc5A2wQAAAGwGGNMIDv87W9/U1VVlVwul9555x298cYbKi4ubjYUfNu1a9c0fPhwZWRkaPXq1c3WeTweeTwe32232y2n0ymXyyWbzRZIuwAA3NHcbrfsdnur76EBnUMgSWFhYRoyZIgkaezYsfrkk0+0fv16bdy4sdV9e/furdGjR+vs2bMt1lmtVlmt1kBbAwAAQWr3dQi8Xq/fv+Zb0tjYqPLycsXExLT3aQEAQAcK6AhBdna2Zs6cqfvuu0/19fXatm2bioqKtH//fklSZmamBg4cqJycHEnSqlWrNHHiRA0ZMkSXL1/W2rVrdf78eS1atKjjZwIAAIIWUCC4ePGiMjMzVVNTI7vdrqSkJO3fv1/Tpk2TJFVVVSkk5JuDDpcuXdLixYtVW1uryMhIjR07ViUlJW063wAAANw6AZ9U2BXaekIEAADw19b3UH7LAAAAEAgAAACBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAAAgAgEAABCBAAAAiEAAAABEIAAAACIQAAAAEQgAAIAIBAAAQAQCAACgAANBbm6ukpKSZLPZZLPZlJycrH379rW4z44dOzRs2DCFh4dr5MiR2rt3b7saBgAAHS+gQDBo0CCtWbNGx44d09GjR/WDH/xAc+bM0cmTJ5usLykpUUZGhhYuXKiysjKlp6crPT1dFRUVHdI8AADoGBZjjGnPA/Tr109r167VwoULb7pv7ty5unLlivbs2eMbmzhxokaNGqUNGzY0+5gej0cej8d32+12y+l0yuVyyWaztaddAADuKG63W3a7vdX30KDPIWhsbFR+fr6uXLmi5OTkJmtKS0uVmprqN5aWlqbS0tIWHzsnJ0d2u923OZ3OYNsEAABtEHAgKC8v19133y2r1aqnn35aO3fuVEJCQpO1tbW1io6O9huLjo5WbW1ti8+RnZ0tl8vl26qrqwNtEwAABCA00B3i4+N14sQJuVwuvfPOO1qwYIGKi4ubDQXBsFqtslqtHfZ4AACgZQEHgrCwMA0ZMkSSNHbsWH3yySdav369Nm7ceFOtw+FQXV2d31hdXZ0cDkeQ7QIAgM7Q7usQeL1evxMAvy05OVkHDhzwGyssLGz2nAMAANA1AjpCkJ2drZkzZ+q+++5TfX29tm3bpqKiIu3fv1+SlJmZqYEDByonJ0eStGzZMk2ePFnr1q3TrFmzlJ+fr6NHj2rTpk0dPxMAABC0gALBxYsXlZmZqZqaGtntdiUlJWn//v2aNm2aJKmqqkohId8cdEhJSdG2bdv0wgsv6Pnnn9fQoUO1a9cuJSYmduwsAABAu7T7OgS3Qlu/QwkAAPx1+nUIAADA7YNAAAAACAQAAIBAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACAAgwEOTk5euihhxQREaGoqCilp6fr9OnTLe6Tl5cni8Xit4WHh7eraQAA0LECCgTFxcXKysrS4cOHVVhYqGvXrmn69Om6cuVKi/vZbDbV1NT4tvPnz7eraQAA0LFCAykuKCjwu52Xl6eoqCgdO3ZM3//+95vdz2KxyOFwBNchAADodO06h8DlckmS+vXr12JdQ0ODYmNj5XQ6NWfOHJ08ebLFeo/HI7fb7bcBAIDOE3Qg8Hq9Wr58uSZNmqTExMRm6+Lj47V582bt3r1bW7duldfrVUpKii5cuNDsPjk5ObLb7b7N6XQG2yYAAGgDizHGBLPjkiVLtG/fPh06dEiDBg1q837Xrl3T8OHDlZGRodWrVzdZ4/F45PF4fLfdbrecTqdcLpdsNlsw7QIAcEdyu92y2+2tvocGdA7BDUuXLtWePXv00UcfBRQGJKl3794aPXq0zp4922yN1WqV1WoNpjUAABCEgD4yMMZo6dKl2rlzpw4ePKi4uLiAn7CxsVHl5eWKiYkJeF8AANA5AjpCkJWVpW3btmn37t2KiIhQbW2tJMlut6tPnz6SpMzMTA0cOFA5OTmSpFWrVmnixIkaMmSILl++rLVr1+r8+fNatGhRB08FAAAEK6BAkJubK0maMmWK3/iWLVv01FNPSZKqqqoUEvLNgYdLly5p8eLFqq2tVWRkpMaOHauSkhIlJCS0r3MAANBhgj6p8FZq6wkRAADAX1vfQ/ktAwAAQCAAAAAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAARCAAAACSQru6ga7Q6DU6UvlXXay/qqiIcI2P66deIZaubgsAgC5zxwWCgooarXz3lGpcV31jMfZwrZidoBmJMV3YGQAAXSegjwxycnL00EMPKSIiQlFRUUpPT9fp06db3W/Hjh0aNmyYwsPDNXLkSO3duzfohtujoKJGS7Ye9wsDklTruqolW4+roKKmS/oCAKCrBRQIiouLlZWVpcOHD6uwsFDXrl3T9OnTdeXKlWb3KSkpUUZGhhYuXKiysjKlp6crPT1dFRUV7W4+EI1eo5XvnpJp4r4bYyvfPaVGb1MVAADc3izGmKDfAf/nf/5HUVFRKi4u1ve///0ma+bOnasrV65oz549vrGJEydq1KhR2rBhQ5P7eDweeTwe32232y2n0ymXyyWbzRZUr6XnvlTG64dbrXt78UQl339PUM8BAEB343a7ZbfbW30Pbde3DFwulySpX79+zdaUlpYqNTXVbywtLU2lpaXN7pOTkyO73e7bnE5ne9qUJF2sv9p6UQB1AADcToIOBF6vV8uXL9ekSZOUmJjYbF1tba2io6P9xqKjo1VbW9vsPtnZ2XK5XL6turo62DZ9oiLCO7QOAIDbSdDfMsjKylJFRYUOHTrUkf1IkqxWq6xWa4c+5vi4foqxh6vWdbXJ8wgskhz2619BBADgThPUEYKlS5dqz549+vDDDzVo0KAWax0Oh+rq6vzG6urq5HA4gnnqoPUKsWjF7ARJ19/8v+3G7RWzE7geAQDgjhRQIDDGaOnSpdq5c6cOHjyouLi4VvdJTk7WgQMH/MYKCwuVnJwcWKcdYEZijHLnj5HD7v+xgMMertz5Y7gOAQDgjhXQRwZZWVnatm2bdu/erYiICN95AHa7XX369JEkZWZmauDAgcrJyZEkLVu2TJMnT9a6des0a9Ys5efn6+jRo9q0aVMHT6VtZiTGaFqCgysVAgDwLQF97dBiafpNc8uWLXrqqackSVOmTNH3vvc95eXl+e7fsWOHXnjhBX322WcaOnSofvnLX+qxxx5rc5Nt/coEAADw19b30HZdh+BWIRAAABCcW3IdAgAAcHsgEAAAAAIBAAAgEAAAABEIAACA2nHp4lvpxhch3G53F3cCAEDPcuO9s7UvFfaIQFBfXy9JHfKrhwAA3Inq6+tlt9ubvb9HXIfA6/Xqiy++UERERLMXRwqU2+2W0+lUdXX1bXNtA+bU/d1u85GYU0/BnHqGzpiTMUb19fUaMGCAQkKaP1OgRxwhCAkJafVHlIJls9lum79INzCn7u92m4/EnHoK5tQzdPScWjoycAMnFQIAAAIBAAC4gwOB1WrVihUrZLVau7qVDsOcur/bbT4Sc+opmFPP0JVz6hEnFQIAgM51xx4hAAAA3yAQAAAAAgEAACAQAAAAEQgAAIBuk0Dw0Ucfafbs2RowYIAsFot27drV6j5FRUUaM2aMrFarhgwZory8vJtqXnvtNX3ve99TeHi4JkyYoCNHjnR8880IdE5/+MMfNG3aNPXv3182m03Jycnav3+/X82LL74oi8Xitw0bNqwTZ+Ev0DkVFRXd1K/FYlFtba1fXU9ap6eeeqrJOY0YMcJX05XrlJOTo4ceekgRERGKiopSenq6Tp8+3ep+O3bs0LBhwxQeHq6RI0dq7969fvcbY/Sv//qviomJUZ8+fZSamqozZ8501jT8BDOn119/XY888ogiIyMVGRmp1NTUm/5eNbWWM2bM6Myp+AQzp7y8vJv6DQ8P96vpqnUKZj5Tpkxp8rU0a9YsX01XrlFubq6SkpJ8VxxMTk7Wvn37Wtynq19Ht0UguHLlih588EG99tprbaqvrKzUrFmz9Oijj+rEiRNavny5Fi1a5PcG+p//+Z/6p3/6J61YsULHjx/Xgw8+qLS0NF28eLGzpuEn0Dl99NFHmjZtmvbu3atjx47p0Ucf1ezZs1VWVuZXN2LECNXU1Pi2Q4cOdUb7TQp0TjecPn3ar+eoqCjffT1tndavX+83l+rqavXr108/+tGP/Oq6ap2Ki4uVlZWlw4cPq7CwUNeuXdP06dN15cqVZvcpKSlRRkaGFi5cqLKyMqWnpys9PV0VFRW+ml/+8pf6zW9+ow0bNuiPf/yj7rrrLqWlpenq1avdck5FRUXKyMjQhx9+qNLSUjmdTk2fPl2ff/65X92MGTP81untt9/u7OlICm5O0vXL4X673/Pnz/vd31XrFMx8/vCHP/jNpaKiQr169brptdRVazRo0CCtWbNGx44d09GjR/WDH/xAc+bM0cmTJ5us7xavI3ObkWR27tzZYs3PfvYzM2LECL+xuXPnmrS0NN/t8ePHm6ysLN/txsZGM2DAAJOTk9Oh/bZFW+bUlISEBLNy5Urf7RUrVpgHH3yw4xprh7bM6cMPPzSSzKVLl5qt6enrtHPnTmOxWMxnn33mG+tO63Tx4kUjyRQXFzdb8/jjj5tZs2b5jU2YMMH8wz/8gzHGGK/XaxwOh1m7dq3v/suXLxur1Wrefvvtzmm8BW2Z03f93//9n4mIiDBvvfWWb2zBggVmzpw5ndBh4Noypy1bthi73d7s/d1pnYJZo1deecVERESYhoYG31h3WiNjjImMjDRvvPFGk/d1h9fRbXGEIFClpaVKTU31G0tLS1Npaakk6W9/+5uOHTvmVxMSEqLU1FRfTXfn9XpVX1+vfv36+Y2fOXNGAwYM0ODBgzVv3jxVVVV1UYdtN2rUKMXExGjatGn6+OOPfeO3wzq9+eabSk1NVWxsrN94d1knl8slSTf9Pfq21l5PlZWVqq2t9aux2+2aMGFCl6xTW+b0XV999ZWuXbt20z5FRUWKiopSfHy8lixZoi+//LJDe22rts6poaFBsbGxcjqdN/1rtTutUzBr9Oabb+qJJ57QXXfd5TfeHdaosbFR+fn5unLlipKTk5us6Q6vozsyENTW1io6OtpvLDo6Wm63W19//bX+93//V42NjU3WfPfz6+7q5ZdfVkNDgx5//HHf2IQJE5SXl6eCggLl5uaqsrJSjzzyiOrr67uw0+bFxMRow4YN+v3vf6/f//73cjqdmjJlio4fPy5JPX6dvvjiC+3bt0+LFi3yG+8u6+T1erV8+XJNmjRJiYmJzdY193q6sQY3/tsd1qmtc/qun//85xowYIDf/4xnzJih3/3udzpw4IBeeuklFRcXa+bMmWpsbOyM1pvV1jnFx8dr8+bN2r17t7Zu3Sqv16uUlBRduHBBUvdZp2DW6MiRI6qoqLjptdTVa1ReXq67775bVqtVTz/9tHbu3KmEhIQma7vD66hH/PwxArNt2zatXLlSu3fv9vu8febMmb4/JyUlacKECYqNjdX27du1cOHCrmi1RfHx8YqPj/fdTklJ0blz5/TKK6/o3//937uws47x1ltvqW/fvkpPT/cb7y7rlJWVpYqKilt6nklnC2ZOa9asUX5+voqKivxOwnviiSd8fx45cqSSkpJ0//33q6ioSFOnTu3QvlvS1jklJyf7/es0JSVFw4cP18aNG7V69erObrPNglmjN998UyNHjtT48eP9xrt6jeLj43XixAm5XC698847WrBggYqLi5sNBV3tjjxC4HA4VFdX5zdWV1cnm82mPn366N5771WvXr2arHE4HLey1YDl5+dr0aJF2r59+02Hn76rb9++euCBB3T27Nlb1F37jR8/3tdvT14nY4w2b96sJ598UmFhYS3WdsU6LV26VHv27NGHH36oQYMGtVjb3Ovpxhrc+G9Xr1Mgc7rh5Zdf1po1a/T+++8rKSmpxdrBgwfr3nvv7bbr9F29e/fW6NGjff12h3UKZj5XrlxRfn5+m8LyrV6jsLAwDRkyRGPHjlVOTo4efPBBrV+/vsna7vA6uiMDQXJysg4cOOA3VlhY6EvPYWFhGjt2rF+N1+vVgQMHmv38pzt4++239eMf/1hvv/2231dvmtPQ0KBz584pJibmFnTXMU6cOOHrt6euk3T9rOqzZ8+26X9it3KdjDFaunSpdu7cqYMHDyouLq7VfVp7PcXFxcnhcPjVuN1u/fGPf7wl6xTMnKTrZ3SvXr1aBQUFGjduXKv1Fy5c0Jdfftlt1+m7GhsbVV5e7uu3K9epPfPZsWOHPB6P5s+f32rtrVyjpni9Xnk8nibv6xavow45NbGL1dfXm7KyMlNWVmYkmV/96lemrKzMnD9/3hhjzHPPPWeefPJJX/2nn35q/u7v/s48++yz5s9//rN57bXXTK9evUxBQYGvJj8/31itVpOXl2dOnTpl/v7v/9707dvX1NbWdss5/cd//IcJDQ01r732mqmpqfFtly9f9tX88z//sykqKjKVlZXm448/Nqmpqebee+81Fy9e7JZzeuWVV8yuXbvMmTNnTHl5uVm2bJkJCQkxH3zwga+mp63TDfPnzzcTJkxo8jG7cp2WLFli7Ha7KSoq8vt79NVXX/lqnnzySfPcc8/5bn/88ccmNDTUvPzyy+bPf/6zWbFihendu7cpLy/31axZs8b07dvX7N692/zpT38yc+bMMXFxcebrr7/ulnNas2aNCQsLM++8847fPvX19caY6+v+zDPPmNLSUlNZWWk++OADM2bMGDN06FBz9erVbjmnlStXmv3795tz586ZY8eOmSeeeMKEh4ebkydP+s27K9YpmPnc8PDDD5u5c+feNN7Va/Tcc8+Z4uJiU1lZaf70pz+Z5557zlgsFvP+++83OZ/u8Dq6LQLBja+nfXdbsGCBMeb6V08mT5580z6jRo0yYWFhZvDgwWbLli03Pe5vf/tbc99995mwsDAzfvx4c/jw4c6fzLf6C2ROkydPbrHemOtfrYyJiTFhYWFm4MCBZu7cuebs2bPddk4vvfSSuf/++014eLjp16+fmTJlijl48OBNj9uT1smY618V6tOnj9m0aVOTj9mV69TUXCT5vT4mT57s9/fKGGO2b99uHnjgARMWFmZGjBhh3nvvPb/7vV6v+Zd/+RcTHR1trFarmTp1qjl9+vQtmFFwc4qNjW1ynxUrVhhjjPnqq6/M9OnTTf/+/U3v3r1NbGysWbx48S0LosHMafny5b7XSXR0tHnsscfM8ePH/R63q9Yp2L93f/nLX4wk35vst3X1Gv3kJz8xsbGxJiwszPTv399MnTrVr8/u+DqyGGNMxxxrAAAAPdUdeQ4BAADwRyAAAAAEAgAAQCAAAAAiEAAAABEIAACACAQAAEAEAgAAIAIBAAAQgQAAAIhAAAAAJP0/EhLDTYfR1gQAAAAASUVORK5CYII=\n"},"metadata":{}}]},{"cell_type":"code","source":["# y = Wx + b    # -1 ~ 1 범위 내에서 무작위로 선택\n","model = nn.Linear(1,1)      # 직선 그어주는 방정식을 세울 수 있음 -> (입력 특성의 수, 출력 특성의 수)\n","print(model)\n","\n","# Linear(in_features=1, out_features=1, bias=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OjFBw1M0-S4c","executionInfo":{"status":"ok","timestamp":1753407859182,"user_tz":-540,"elapsed":43,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"74694101-f27a-48fd-caf1-95f6e7dede41"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear(in_features=1, out_features=1, bias=True)\n"]}]},{"cell_type":"code","source":["# model가지고 예측\n","y_pred = model(x_train)\n","print(y_pred)\n","# 기울기 조정한 적이 없기때문에, 값이 엉망으로 나옴"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-I7im4H-yXB","executionInfo":{"status":"ok","timestamp":1753407859213,"user_tz":-540,"elapsed":26,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"30cad2cc-c574-4c49-8b89-75cdaca0e949"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1.2410],\n","        [1.6109],\n","        [1.9809]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"code","source":["print(list(model.parameters()))\n","# 기울기가 나중에 미분되면서 변경할 수 있게 수식을 기억하고있겠다! = requires_grad=True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0CRAiOea_C5j","executionInfo":{"status":"ok","timestamp":1753407859390,"user_tz":-540,"elapsed":172,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"a12efc49-e08a-4f8f-fe9e-75d44e1f9a63"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[0.3699]], requires_grad=True), Parameter containing:\n","tensor([0.8711], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["# 값 직접 계산하기 , y = Wx +b\n","print(0.3699*1+ 0.8711)\n","print(0.3699*2+ 0.8711)\n","print(0.3699*3+ 0.8711)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pi7S4FFAAGaO","executionInfo":{"status":"ok","timestamp":1753407859392,"user_tz":-540,"elapsed":89,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"61368723-0298-469d-cc98-500064124937"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["1.241\n","1.6109\n","1.9808000000000001\n"]}]},{"cell_type":"markdown","source":["### 손실 함수\n","\n","손실 함수(Loss Function)는 머신러닝과 딥러닝 모델이 예측한 값과 실제 값 사이의 차이를 수치적으로 나타내는 함수입니다. 모델이 학습을 통해 최적의 결과를 도출하려면 이 차이를 최소화해야 합니다. 손실 함수는 예측값과 실제값의 오차를 계산하여 하나의 숫자(스칼라 값)로 반환하며, 이 값은 비용(Cost) 또는 오차(Error)라고도 불립니다. 예를 들어, 회귀 문제에서는 주로 평균 제곱 오차(MSE, Mean Squared Error)를 사용하여 예측값과 실제값 간의 평균적인 차이를 측정하고, 분류 문제에서는 교차 엔트로피 손실(Cross-Entropy Loss)을 사용해 예측 확률 분포와 실제 레이블 분포 간의 차이를 계산합니다. 손실 함수가 반환한 값은 역전파(Backpropagation)를 통해 모델의 가중치와 편향을 조정하는 데 사용됩니다. 즉, 손실 함수는 모델이 학습 과정에서 목표로 삼아야 할 방향을 알려주는 나침반 역할을 합니다.\n","\n","- MSE 값이 작을수록 모델의 예측 정확도가 높다고 판단할 수 있다.\n","\n","<img src='https://blog.kakaocdn.net/dna/LIEFi/btsPxTEnIqk/AAAAAAAAAAAAAAAAAAAAAL6QWr332q9da-L7FgtxeHG-oi7ym6QJbPBH30HF7OCN/img.webp?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=mR8sOT0QxNbcUa99%2FYfKPaL%2FgUQ%3D'>\n","\n","단 , MAE는 잘 쓰지 않음 . 오류 측정에는 용이하지만 기울기 변화의 근거로 사용하기에는 좋지 않음\n","왜? 미분을 해서 반대로 값을 돌려보는데, 절댓값 미분이 쉽지 않음\n","\n","> MSE와 분산의 관계\n","MSE는 분산과 편향(Bias)의 합으로 표현될 수 있습니다. 즉, MSE를 줄이기 위해서는 분산과 편향을 모두 줄여야 합니다."],"metadata":{"id":"cfpY9syfAUHV"}},{"cell_type":"code","source":["((y_pred - y_train)**2).mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MThGIFM2AQ3V","executionInfo":{"status":"ok","timestamp":1753407859396,"user_tz":-540,"elapsed":29,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"939bf943-0a18-4a04-d417-b6dbd0798b2b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7.4790, grad_fn=<MeanBackward0>)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# 파이토치 내장함수 nn.MSELoss()()\n","loss = nn.MSELoss()(y_pred, y_train)\n","loss\n","# tensor(7.4790, grad_fn=<MseLossBackward0>)\n","\n","# __call()__ 실행한 것처럼\n","# mse_loss = nn.MSELoss()mse_loss(y_pred, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IaUqkMgMC_Ee","executionInfo":{"status":"ok","timestamp":1753407859423,"user_tz":-540,"elapsed":25,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"9c2fd7cd-f39c-41ba-8156-6945b6d9b26a"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(7.4790, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### 최적화\n","\n","최적화(Optimization)는 주어진 목표를 달성하기 위해 최상의 해결책(Optimal Solution)을 찾아가는 과정입니다. 머신러닝과 딥러닝에서는 주로 모델이 예측한 값과 실제 값 사이의 오차(손실 함수 값)를 최소화하는 것을 목표로 합니다. 이 과정에서 모델의 학습 가능한 파라미터(가중치와 편향)를 조정하여 손실 함수의 값을 점점 더 작게 만들어갑니다.\n","\n","최적화는 주로 경사 하강법(Gradient Descent)과 같은 알고리즘을 사용해 수행되며, 손실 함수의 기울기(Gradient)를 따라가며 최저점(또는 최적점)을 찾습니다. 이 과정은 마치 산에서 가장 낮은 지점을 찾아 내려가는 것과 비슷합니다. 최적화는 단순히 손실을 줄이는 것뿐만 아니라, 학습 속도, 안정성, 과적합 방지와 같은 다양한 요소를 고려해야 하는 복합적인 과정입니다.\n","\n","즉, 최적화는 모델이 데이터로부터 가장 정확하고 효율적인 예측을 할 수 있도록 파라미터를 조정하는 핵심 과정입니다."],"metadata":{"id":"Jbr28NU2D4uG"}},{"cell_type":"markdown","source":["## 경사하강법\n","\n","경사하강법(Gradient Descent)은 머신러닝과 딥러닝 모델이 최적의 가중치(Weights)와 편향(Biases)를 찾기 위해 손실 함수(Loss Function)를 최소화하는 방법입니다. 이 알고리즘은 마치 산 꼭대기에서 출발해 가장 낮은 지점(최솟값)을 찾아 내려가는 과정과 비슷합니다.\n","\n","먼저, 모델은 손실 함수의 기울기(Gradient)를 계산합니다. 이 기울기는 현재 지점에서 손실이 가장 빠르게 감소하는 방향을 나타냅니다. 이후, 모델은 기울기의 반대 방향으로 가중치와 편향 값을 조금씩 업데이트합니다. 이때 학습률(Learning Rate)은 한 번에 이동하는 \"걸음의 크기\"를 결정합니다. 학습률이 너무 크면 최적의 지점을 지나칠 수 있고, 너무 작으면 학습 속도가 매우 느려질 수 있습니다. 이 과정을 반복하면서 손실 함수 값이 점점 작아지고, 결국 최적의 가중치와 편향을 찾아내게 됩니다.\n","\n","즉, 경사하강법은 모델이 더 나은 예측을 할 수 있도록 가중치를 조정해주는 핵심 최적화 알고리즘입니다.\n","\n","경사하강법은 데이터를 어떻게 나눠서 학습하느냐에 따라 배치(Batch)= 일반 경사하강법, 확률적(Stochastic), 미니배치(Mini-Batch)로 나뉩니다.\n","\n","<img src=\"https://blog.kakaocdn.net/dna/deKpeG/btsPxzsHJXU/AAAAAAAAAAAAAAAAAAAAAIWmDKutVPEH0ZP17BmczCQ6vtjFqolLNKz8VxGv23B5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=jeQ7D1FiVO%2BaoQ6Fmz8Ft%2FrFIKA%3D\">"],"metadata":{"id":"GE3Hf1v4D6rS"}},{"cell_type":"markdown","source":["### 학습률\n","\n","학습률(Learning Rate)은 머신러닝과 딥러닝 모델이 학습할 때 가중치(Weights)와 편향(Biases)를 얼마나 크게 조정할지를 결정하는 하이퍼파라미터입니다. 경사하강법(Gradient Descent)과 같은 최적화 알고리즘에서 손실 함수(Loss Function)의 기울기(Gradient)를 따라 최적의 가중치를 찾아갈 때, 학습률은 한 번의 업데이트에서 이동하는 \"걸음의 크기\"를 의미합니다. 학습률이 너무 크면 최적의 가중치를 지나쳐 버리거나 학습이 불안정해질 수 있고, 너무 작으면 학습 속도가 매우 느려져 최적값에 도달하기 어려울 수 있습니다. 따라서 적절한 학습률을 선택하는 것은 모델의 학습 속도와 최적화 성능을 결정하는 중요한 요소입니다. 일반적으로 고정된 학습률을 사용하기도 하지만, 상황에 따라 학습률을 점진적으로 줄이거나 동적으로 조정하는 방법(예: Adam, Step Decay, Cyclical Learning Rate 등)이 사용되기도 합니다.\n","\n","<img src=\"https://blog.kakaocdn.net/dna/vNIf5/btsPxX0TB7d/AAAAAAAAAAAAAAAAAAAAANyZRBGL5n-oH2qeQGVBZr4uVyAzZFBGijLEA70w-1Bo/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=joVdqpGWI6%2BZkMmtNa6hJp2EuVI%3D\">\n"],"metadata":{"id":"ff6lMB7pEYNy"}},{"cell_type":"markdown","source":["## 가중치 업데이트\n","\n","<img src=\"https://blog.kakaocdn.net/dna/EEeJr/btsPyFSGC9P/AAAAAAAAAAAAAAAAAAAAAMDPFEZyzzBAaQIo1uzopmoJep4LLE19iX92iPBD83Ag/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=oILenGvqU9Grrk6ykjbjAKR%2BXDI%3D\">\n"],"metadata":{"id":"Byl9hNKtKg93"}},{"cell_type":"markdown","source":["## 편향(Bias) 업데이트\n","\n","<img src=\"https://blog.kakaocdn.net/dna/bNhSoN/btsPxd4vc3Q/AAAAAAAAAAAAAAAAAAAAAMrF1Y8NycKMsH9n-DS88HrSvnFtOkeWASAAVH_TfUNf/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=wNLaQga%2Fs8nQOFZgXVmk2%2FHzZSc%3D\">\n"],"metadata":{"id":"U_bl0fzpKnH4"}},{"cell_type":"markdown","source":["## 가중치 업데이트 예\n","\n","<img src=\"https://blog.kakaocdn.net/dna/crGLYC/btsPw3t0los/AAAAAAAAAAAAAAAAAAAAAHpENHdxGQsfEBVaGb_30YxpHEAwsSQKUAocrHqPQtpm/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=jwdvRvK2b0WY4bsM9QQg25DPVU8%3D\">\n"],"metadata":{"id":"NEIzZQCpKnTN"}},{"cell_type":"markdown","source":["### 확률적 경사하강법\n","\n","SGD(Stochastic Gradient Descent, 확률적 경사하강법)는 모델의 가중치(Weights)와 편향(Biases)를 최적화하기 위해 손실 함수(Loss Function)의 기울기(Gradient)를 따라 반복적으로 업데이트하는 가장 기본적인 옵티마이저입니다. 일반적인 경사하강법은 전체 데이터셋을 한 번에 사용해 기울기를 계산하지만, SGD는 무작위로 선택된 하나의 데이터 포인트(순수 SGD) 또는 작은 그룹(미니배치 SGD)을 사용해 기울기를 계산하고 가중치를 조정합니다. 이로 인해 학습 속도가 빨라지고 메모리 사용량이 줄어들지만, 진동이 발생할 수 있어 학습이 불안정할 수도 있습니다. PyTorch에서 SGD는 optim.SGD로 구현되며, 학습률(lr)과 모멘텀(momentum) 등의 매개변수를 통해 조정할 수 있습니다. 주로 작은 데이터셋이나 빠른 반복 학습이 필요한 경우 사용되며, 학습률이 적절하게 설정되면 강력하고 효율적인 최적화 결과를 제공합니다."],"metadata":{"id":"pyevYDlmLrX1"}},{"cell_type":"code","source":["# model.parameters : 모델에 무작위 weight와 bias 넣어주는\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"metadata":{"id":"eEhYVJt1Ltke","executionInfo":{"status":"ok","timestamp":1753407878448,"user_tz":-540,"elapsed":48,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 공식처럼 쓰일 파이토치 코드\n","\n","# gradient를 초기화\n","optimizer.zero_grad()\n","\n","# 역전파 : 비용 함수를 미분하여 gradient(기울기) 계산\n","loss.backward()\n","\n","# W와 b를 업데이트\n","optimizer.steop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"VSn_LDpmMYFU","executionInfo":{"status":"error","timestamp":1753407881438,"user_tz":-540,"elapsed":124,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"4d7880a8-41c3-4a9a-d446-5b6884d1fb8c"},"execution_count":17,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-17-2177636592.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 역전파 : 비용 함수를 미분하여 gradient(기울기) 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# W와 b를 업데이트\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."]}]},{"cell_type":"code","source":["print(list(model.parameters()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXzylpR6MzV0","executionInfo":{"status":"ok","timestamp":1753407871900,"user_tz":-540,"elapsed":28,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"fac2863e-027e-4142-ed1a-eb290595aed0"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[Parameter containing:\n","tensor([[0.3699]], requires_grad=True), Parameter containing:\n","tensor([0.8711], requires_grad=True)]\n"]}]},{"cell_type":"code","source":["'''\n","[Parameter containing:\n","tensor([[0.3699]], requires_grad=True), Parameter containing:\n","tensor([0.8711], requires_grad=True)]\n","------------ 학습 전 후 -------------\n","[Parameter containing:\n","tensor([[0.3699]], requires_grad=True), Parameter containing:\n","tensor([0.8711], requires_grad=True)]\n","'''"],"metadata":{"id":"4MCqLjLtM3BU","executionInfo":{"status":"aborted","timestamp":1753407866354,"user_tz":-540,"elapsed":16416,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 반복 학습을 통해 오차가 있는 W, b를 수정하면서 오차를 계속 줄여나감\n","# epochs: 반복 학습 횟수(에포크)\n","epochs = 1000\n","\n","for epoch in range(epochs + 1):\n","    # 핵심코드!\n","    y_pred = model(x_train)\n","    loss = nn.MSELoss()(y_pred, y_train)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","        print(f'Epoch: {epoch}/{epochs} Loss: {loss:.6f}')\n","\n","print(list(model.parameters()))\n","\n","x_test = torch.FloatTensor([[5]])\n","y_pred = model(x_test)\n","print(y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GhUBdOePNf99","executionInfo":{"status":"ok","timestamp":1753407873751,"user_tz":-540,"elapsed":425,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"e695a010-e2f0-4324-e80e-a1ab602f8c8b"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0/1000 Loss: 7.478992\n","Epoch: 100/1000 Loss: 0.156957\n","Epoch: 200/1000 Loss: 0.096990\n","Epoch: 300/1000 Loss: 0.059934\n","Epoch: 400/1000 Loss: 0.037035\n","Epoch: 500/1000 Loss: 0.022886\n","Epoch: 600/1000 Loss: 0.014142\n","Epoch: 700/1000 Loss: 0.008739\n","Epoch: 800/1000 Loss: 0.005400\n","Epoch: 900/1000 Loss: 0.003337\n","Epoch: 1000/1000 Loss: 0.002062\n","[Parameter containing:\n","tensor([[1.9474]], requires_grad=True), Parameter containing:\n","tensor([0.1196], requires_grad=True)]\n","tensor([[9.8565]], grad_fn=<AddmmBackward0>)\n"]}]},{"cell_type":"markdown","source":["## 3. 다중 선형 회귀\n","다중 선형 회귀(Multiple Linear Regression)는 여러 개의 독립 변수(입력 변수)를 사용해 하나의 종속 변수(출력 변수)를 예측하는 통계 및 머신러닝 기법입니다. 단순 선형 회귀가 하나의 독립 변수와 하나의 종속 변수 간의 선형 관계를 설명하는 반면, 다중 선형 회귀는 두 개 이상의 입력 변수가 출력 변수에 어떻게 영향을 미치는지를 분석합니다. 이 관계는 수식으로 표현되며, 예를 들어 Y=W1X1+W2X2+...+WnXn+b와 같이 나타납니다. 여기서 Y는 예측 값, X1,X2,...Xn ​은 입력 변수, W1,W2,...Wn ​은 각 변수의 가중치, b는 절편입니다. 다중 선형 회귀는 입력 변수들이 독립적이고, 종속 변수와 선형 관계를 가진다는 가정 하에 작동하며, 주로 경제학, 의료, 마케팅 등 다양한 분야에서 복합적인 요인의 영향을 분석하고 예측하는 데 사용됩니다."],"metadata":{"id":"hbYU4vH_O92J"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vskbvI_vPBzR","executionInfo":{"status":"ok","timestamp":1753407868438,"user_tz":-540,"elapsed":46,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"58fad507-f8ab-495b-d4c2-d40cf3ec3e07"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["device: cpu\n"]}]},{"cell_type":"code","source":["X_train = torch.FloatTensor([[73, 80, 75],\n","                             [93, 88, 93],\n","                             [89, 91, 90],\n","                             [96, 98, 100],\n","                             [73, 66, 70],\n","                             [85, 90, 88],\n","                             [78, 85, 82]]).to(device)\n","\n","y_train = torch.FloatTensor([[152], [185], [180], [196], [142], [175], [155]]).to(device)\n","\n","print(X_train, X_train.shape)\n","print(y_train, y_train.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"166INojYQKSs","executionInfo":{"status":"ok","timestamp":1753407984688,"user_tz":-540,"elapsed":28,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"b66ee726-6319-4520-978f-92b7cc213389"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 73.,  80.,  75.],\n","        [ 93.,  88.,  93.],\n","        [ 89.,  91.,  90.],\n","        [ 96.,  98., 100.],\n","        [ 73.,  66.,  70.],\n","        [ 85.,  90.,  88.],\n","        [ 78.,  85.,  82.]]) torch.Size([7, 3])\n","tensor([[152.],\n","        [185.],\n","        [180.],\n","        [196.],\n","        [142.],\n","        [175.],\n","        [155.]]) torch.Size([7, 1])\n"]}]},{"cell_type":"code","source":["model = nn.Linear(3, 1).to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bHRyyna8Qp6J","executionInfo":{"status":"ok","timestamp":1753407994593,"user_tz":-540,"elapsed":60,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"87f5d8d1-f024-449e-c77c-f87ff249310d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear(in_features=3, out_features=1, bias=True)\n"]}]},{"cell_type":"code","source":["# optimizer = optim.SGD(model.parameters(), lr=0.00001)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","loss_fn = nn.MSELoss()"],"metadata":{"id":"KZDs9nN5Qslg","executionInfo":{"status":"ok","timestamp":1753408609921,"user_tz":-540,"elapsed":40,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["## Adam\n","Adam(Adaptive Moment Estimation)은 경사하강법(Gradient Descent)을 개선한 최적화 알고리즘으로, 머신러닝과 딥러닝 모델 학습에서 널리 사용됩니다. Adam은 SGD(Stochastic Gradient Descent)의 단점을 보완하기 위해 개발되었으며, 각 가중치(Weight)와 편향(Bias)마다 다른 학습률(Learning Rate)을 적용해 더욱 효율적이고 안정적으로 최적화를 수행합니다.\n","\n","Adam은 다음 두 가지 아이디어를 결합한 최적화 알고리즘입니다.\n","\n","<img src=\"https://blog.kakaocdn.net/dna/c1Rgss/btsPzl7jQwg/AAAAAAAAAAAAAAAAAAAAANLHrhOs4Zs0rOwPKzaKhc6MZSWkfkeWowXWk_UU0CR8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1753973999&allow_ip=&allow_referer=&signature=pCnW%2Bz6OcZc9l0WY2frNfl8qM%2FQ%3D\">"],"metadata":{"id":"CobTXiuERO1C"}},{"cell_type":"markdown","source":["## Adam이 필요한 이유\n","\n","```r\n","W = W - learning_rate × gradient\n","```\n","기본적인 경사 하강법(Gradient Descent)은 학습이 위와 같이 이루어집니다. 하지만 이 방식은 다음과 같은 문제가 있습니다\n","\n","- 학습 속도가 느림\n","- local minimum에 빠짐\n","- 데이터에 따라 진동이 심할 수 있음\n","- 모든 파라미터에 똑같은 learning rate 사용\n","\n","→ 이를 개선하기 위해 나온 게 Adam입니다!\n","\n"],"metadata":{"id":"LdOxGXvkReOg"}},{"cell_type":"markdown","source":["### **수식**\n","```r\n","1. m ← β1 * m + (1 - β1) * gradient  ← 1차 모멘트 (평균)\n","2. v ← β2 * v + (1 - β2) * (gradient)^2  ← 2차 모멘트 (분산)\n","3. m̂ ← m / (1 - β1^t)   ← 편향 보정\n","4. v̂ ← v / (1 - β2^t)\n","5. W = W - learning_rate × (m̂ / (√v̂ + ε))\n","```\n","- m: 평균처럼 누적된 기울기\n","- v: 분산처럼 누적된 제곱 기울기\n","- β1, β2: 모멘텀 정도를 조절하는 하이퍼파라미터 (보통 0.9, 0.999 사용)\n","- ε(엡실론): 0으로 나누는 것을 방지하기 위한 아주 작은 값 (보통 1e-8)"],"metadata":{"id":"rF12Yz2iSTX_"}},{"cell_type":"code","source":["epochs = 1000\n","\n","for epoch in range(epochs + 1):\n","    # 지금은 엉망\n","    y_pred = model(X_train)\n","    # 로스를 구해라\n","    loss = loss_fn(y_pred, y_train)\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    if epoch % 100 == 0:\n","        print(f'Epoch: {epoch}/{epochs}, Loss: {loss.item():.6f}')\n","\n","print(\"\\n모델 파라미터:\")\n","for param in model.parameters():\n","    print(param)\n","\n","x_test = torch.FloatTensor([[93, 93, 93]]).to(device)\n","y_pred = model(x_test)\n","print(f\"\\n새로운 입력 데이터 {x_test.tolist()}의 예측 결과: {y_pred.item():.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JmnUzOzEQzlI","executionInfo":{"status":"ok","timestamp":1753408612874,"user_tz":-540,"elapsed":537,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"}},"outputId":"63512e9a-b24b-42ba-fda2-6d0a5aa9d258"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 0/1000, Loss: 11.068503\n","Epoch: 100/1000, Loss: 8.290606\n","Epoch: 200/1000, Loss: 6.866682\n","Epoch: 300/1000, Loss: 6.495923\n","Epoch: 400/1000, Loss: 6.341359\n","Epoch: 500/1000, Loss: 6.207748\n","Epoch: 600/1000, Loss: 6.077526\n","Epoch: 700/1000, Loss: 5.950419\n","Epoch: 800/1000, Loss: 5.826257\n","Epoch: 900/1000, Loss: 5.744383\n","Epoch: 1000/1000, Loss: 5.602735\n","\n","모델 파라미터:\n","Parameter containing:\n","tensor([[1.2714, 0.3077, 0.4455]], requires_grad=True)\n","Parameter containing:\n","tensor([-1.6448], requires_grad=True)\n","\n","새로운 입력 데이터 [[93.0, 93.0, 93.0]]의 예측 결과: 186.6441\n"]}]}]}