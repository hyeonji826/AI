{"cells":[{"cell_type":"markdown","metadata":{"id":"1hv7MF7yWqGG"},"source":["## 데이터셋\n","- Tiny ImageNet 사용\n","    - ImageNet을 축소한 버전(200개 클래스, 클래스당 500장 학습 이미지, 64×64 해상도)"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":16442,"status":"ok","timestamp":1754316427057,"user":{"displayName":"Hyeonji","userId":"17627784022328153589"},"user_tz":-540},"id":"ReTe4UwTWlrQ"},"outputs":[],"source":["!wget -q http://cs231n.stanford.edu/tiny-imagenet-200.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIlDVfo-W1mp"},"outputs":[],"source":["!unzip -q tiny-imagenet-200.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeuXlwo4XFaG"},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"P4JQJ-iqXSo6"},"outputs":[],"source":["class TinyImageNet(Dataset):\n","    def __init__(self, root, split='train', transform=None):\n","        self.transform = transform\n","        self.images = []\n","        self.labels = []\n","        wnids = open(os.path.join(root, 'wnids.txt')).read().splitlines()\n","        wnid_to_idx = {wnid: idx for idx, wnid in enumerate(wnids)}\n","\n","        if split == 'train':\n","            train_dir = os.path.join(root, 'train')\n","            for wnid in wnids:\n","                img_dir = os.path.join(train_dir, wnid, 'images')\n","                for name in os.listdir(img_dir):\n","                    self.images.append(os.path.join(img_dir, name))\n","                    self.labels.append(wnid_to_idx[wnid])\n","        else:  # validation\n","            val_dir = os.path.join(root, 'val')\n","            # val_annotations.txt: filename\\twnid\\t...\n","            ann = open(os.path.join(val_dir, 'val_annotations.txt')).read().splitlines()\n","            filename_to_wnid = {line.split('\\t')[0]: line.split('\\t')[1] for line in ann}\n","            for name in os.listdir(os.path.join(val_dir, 'images')):\n","                self.images.append(os.path.join(val_dir, 'images', name))\n","                self.labels.append(wnid_to_idx[filename_to_wnid[name]])\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.images[idx]).convert('RGB')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, self.labels[idx]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O9-SFyXPXYCV"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]}],"source":["# ImageNet 표준 정규화 값\n","imagenet_mean = [0.485, 0.456, 0.406]\n","imagenet_std  = [0.229, 0.224, 0.225]\n","\n","train_transform = transforms.Compose([\n","    transforms.RandomResizedCrop(224),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_mean, imagenet_std),\n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(imagenet_mean, imagenet_std),\n","])\n","\n","root = '/content/tiny-imagenet-200'\n","train_ds = TinyImageNet(root, 'train', transform=train_transform)\n","val_ds   = TinyImageNet(root, 'val',   transform=val_transform)\n","\n","train_loader = DataLoader(train_ds, batch_size=128, shuffle=True, num_workers=4)\n","val_loader   = DataLoader(val_ds,   batch_size=128, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"UM0QeHfzYc-T"},"outputs":[],"source":["class AlexNet(nn.Module):\n","    def __init__(self, num_classes=200):\n","        super().__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 96, 11, stride=4, padding=2), nn.ReLU(inplace=True),\n","            nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2),\n","            nn.MaxPool2d(3, stride=2),\n","\n","            nn.Conv2d(96, 256, 5, padding=2, groups=2), nn.ReLU(inplace=True),\n","            nn.LocalResponseNorm(5, alpha=1e-4, beta=0.75, k=2),\n","            nn.MaxPool2d(3, stride=2),\n","\n","            nn.Conv2d(256, 384, 3, padding=1), nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 384, 3, padding=1, groups=2), nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, 3, padding=1, groups=2), nn.ReLU(inplace=True),\n","            nn.MaxPool2d(3, stride=2),\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(256*6*6, 4096), nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),     nn.ReLU(inplace=True),\n","            nn.Linear(4096, num_classes),\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        return self.classifier(x)\n","\n","def build_alexnet(num_classes=200):\n","    return AlexNet(num_classes)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"o3jvAqrlXs39"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"STT60KViYHMw"},"outputs":[],"source":["model = AlexNet(num_classes=200).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bv1GZlcvYKSj"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kuZsZTWaZHTH"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]}],"source":["scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g_VAgrjYYM3o"},"outputs":[],"source":["num_epochs = 20\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    total, correct = 0, 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = outputs.max(1)\n","            total += labels.size(0)\n","            correct += (preds == labels).sum().item()\n","\n","    val_acc = correct / total\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Val Accuracy: {val_acc:.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNSvN+y0Q8DGNK1xmqrnfsb","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}